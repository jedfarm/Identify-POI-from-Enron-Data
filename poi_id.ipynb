{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tester import dump_classifier_and_data\n",
    "import tester\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"rb\"))\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that all the email-related data were collected using valid email addresses, therefore if some people have their email addresses missing, that implies the corresponding email features will be missing as well (NaN). The contrary is not true as we are about to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with missing email data:  60\n",
      "Number of people with missing email address:  35\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of people with missing email data: \",df['from_messages'].value_counts().max())\n",
    "print(\"Number of people with missing email address: \",df['email_address'].value_counts().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are these people?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People with at least one email address but without email-related features: \n",
      "\n",
      "1 ELLIOTT STEVEN\n",
      "2 MORDAUNT KRISTINA M\n",
      "3 WESTFAHL RICHARD K\n",
      "4 WODRASKA JOHN\n",
      "5 ECHOLS JOHN B\n",
      "6 KOPPER MICHAEL J\n",
      "7 BERBERIAN DAVID\n",
      "8 DETMERING TIMOTHY J\n",
      "9 GOLD JOSEPH\n",
      "10 KISHKILL JOSEPH G\n",
      "11 LINDHOLM TOD A\n",
      "12 BUTTS ROBERT H\n",
      "13 HERMANN ROBERT J\n",
      "14 SCRIMSHAW MATTHEW\n",
      "15 FASTOW ANDREW S\n",
      "16 OVERDYKE JR JERE C\n",
      "17 STABLER FRANK\n",
      "18 PRENTICE JAMES\n",
      "19 WHITE JR THOMAS E\n",
      "20 CHRISTODOULOU DIOMEDES\n",
      "21 DIMICHELE RICHARD G\n",
      "22 YEAGER F SCOTT\n",
      "23 HIRKO JOSEPH\n",
      "24 PAI LOU L\n",
      "25 BAY FRANKLIN R\n"
     ]
    }
   ],
   "source": [
    "strange_cases = df.index[(df['email_address'] != 'NaN') & (df['from_messages']=='NaN')].tolist()\n",
    "print(\"People with at least one email address but without email-related features: \")\n",
    "print()\n",
    "for i, name in enumerate(strange_cases):\n",
    "    print(i + 1, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are 25 extrange cases, where actual Enron employees with a valid email address,  do not have email-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People without an email address: \n",
      "\n",
      "1 BAXTER JOHN C\n",
      "2 LOWRY CHARLES P\n",
      "3 WALTERS GARETH W\n",
      "4 CHAN RONNIE\n",
      "5 BELFER ROBERT\n",
      "6 URQUHART JOHN A\n",
      "7 WHALEY DAVID A\n",
      "8 MENDELSOHN JOHN\n",
      "9 CLINE KENNETH W\n",
      "10 WAKEHAM JOHN\n",
      "11 DUNCAN JOHN H\n",
      "12 LEMAISTRE CHARLES\n",
      "13 SULLIVAN-SHAKLOVITZ COLLEEN\n",
      "14 WROBEL BRUCE\n",
      "15 MEYER JEROME J\n",
      "16 CUMBERLAND MICHAEL S\n",
      "17 GAHN ROBERT S\n",
      "18 GATHMANN WILLIAM D\n",
      "19 GILLIS JOHN\n",
      "20 BAZELIDES PHILIP J\n",
      "21 LOCKHART EUGENE E\n",
      "22 PEREIRA PAULO V. FERRAZ\n",
      "23 BLAKE JR. NORMAN P\n",
      "24 GRAY RODNEY\n",
      "25 THE TRAVEL AGENCY IN THE PARK\n",
      "26 NOLES JAMES L\n",
      "27 TOTAL\n",
      "28 JAEDICKE ROBERT\n",
      "29 WINOKUR JR. HERBERT S\n",
      "30 BADUM JAMES P\n",
      "31 REYNOLDS LAWRENCE\n",
      "32 YEAP SOON\n",
      "33 FUGH JOHN L\n",
      "34 SAVAGE FRANK\n",
      "35 GRAMM WENDY L\n"
     ]
    }
   ],
   "source": [
    "emailless_people = df.index[df['email_address'] == 'NaN'].tolist()\n",
    "print(\"People without an email address: \")\n",
    "print()\n",
    "for i, name in enumerate(emailless_people):\n",
    "    print(i + 1, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we found two entities, that are not real people: THE TRAVEL AGENCY IN THE PARK and TOTAL. These entities do not contribute in any meaningful way to the purpose of this study, so let say that from this moment on we mark them for deletion. \n",
    "\n",
    "The email address is the only field that could not be converted to numeric. We chose to remove it from the data frame because it is of no use to identify poi from the data given.\n",
    "Also, in the case of the poi column only zeroes (0) and ones (1) are allowed: 1 = poi, 0 = non-poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.apply(lambda x: pd.to_numeric(x, errors='coerse'))\n",
    "del df['email_address']\n",
    "df['poi']=df['poi'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, METTS MARK to GLISAN JR BEN F\n",
      "Data columns (total 20 columns):\n",
      "poi                          146 non-null int64\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "shared_receipt_with_poi      86 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "dtypes: float64(19), int64(1)\n",
      "memory usage: 24.0+ KB\n"
     ]
    }
   ],
   "source": [
    "poi_label = ['poi']\n",
    "financial_feat_list = ['salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments', \n",
    "                       'loan_advances', 'other', 'expenses', 'director_fees', 'total_payments', \n",
    "                       'exercised_stock_options', 'restricted_stock','restricted_stock_deferred', 'total_stock_value']\n",
    "email_feat_list = ['from_messages', 'from_poi_to_this_person','from_this_person_to_poi', 'shared_receipt_with_poi', \n",
    "                   'to_messages']\n",
    "features_list = poi_label + financial_feat_list + email_feat_list\n",
    "df=df[features_list]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of poi and non-poi in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    128\n",
       "1     18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of poi without email data:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of poi without email data: \", df[(df['poi']==1) & (~df.to_messages.notnull())].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are people without data associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOCKHART EUGENE E</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "LOCKHART EUGENE E    0     NaN    NaN                  NaN              NaN   \n",
       "\n",
       "                   deferral_payments  loan_advances  other  expenses  \\\n",
       "LOCKHART EUGENE E                NaN            NaN    NaN       NaN   \n",
       "\n",
       "                   director_fees  total_payments  exercised_stock_options  \\\n",
       "LOCKHART EUGENE E            NaN             NaN                      NaN   \n",
       "\n",
       "                   restricted_stock  restricted_stock_deferred  \\\n",
       "LOCKHART EUGENE E               NaN                        NaN   \n",
       "\n",
       "                   total_stock_value  from_messages  from_poi_to_this_person  \\\n",
       "LOCKHART EUGENE E                NaN            NaN                      NaN   \n",
       "\n",
       "                   from_this_person_to_poi  shared_receipt_with_poi  \\\n",
       "LOCKHART EUGENE E                      NaN                      NaN   \n",
       "\n",
       "                   to_messages  \n",
       "LOCKHART EUGENE E          NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().sum(axis=1) >= df.shape[1]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the only thing we know about Eugene E. Lockhart is that he is not a person of interest.\n",
    "\n",
    "As we have a relatively low number of data points, we are going to proceed extra-carefully at removing them.\n",
    "For the moment, we are going to do it just to the items we previously have marked for deletion plus this last one, and we will analyze any further need in a case by case manner as we proceed with our ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['TOTAL','THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the financial data, we learned that NaN means zero. Therefore we proceed to make the corresponding changes in our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 1:15] = df.iloc[:, 1:15].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing such an operation, the number of NaN values was dramatically reduced from 1323 up to 285, which ultimately is the amount of missing email-related entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of email-related missing data:  285\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of email-related missing data: \", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the best way to proceed with the remaining NaN values is to impute them with the median for non-poi people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of remaining NaN entries in the dataframe: 0\n"
     ]
    }
   ],
   "source": [
    "df[email_feat_list]=df[email_feat_list].fillna(df.groupby(\"poi\")[email_feat_list].transform(\"median\"))\n",
    "print(\"Amount of remaining NaN entries in the dataframe:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As found in some of our references, the manual input of the financial data could have been the cause of some observed mistakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT        0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  \n",
       "BELFER ROBERT              -44093.0  \n",
       "BHATNAGAR SANJAY                0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payments = financial_feat_list[:9]\n",
    "df[df[payments].sum(axis = 1) != df.total_payments][financial_feat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT        0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  \n",
       "BELFER ROBERT              -44093.0  \n",
       "BHATNAGAR SANJAY                0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_value = financial_feat_list[10:13]\n",
    "test_df=df[df[stock_value].sum(axis='columns') != df.total_stock_value][financial_feat_list]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there are errors in just two rows. Checking the .pdf document obtained from FindLaw, we acknowledged that the errors are in fact shifts of one column in each case but opposite directions. Let's correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the financial data has been corrected\n"
     ]
    }
   ],
   "source": [
    "test_df.loc[['BELFER ROBERT']] = test_df.loc[['BELFER ROBERT']].shift(-1, axis =1).fillna(0)\n",
    "test_df.loc[['BHATNAGAR SANJAY']] = test_df.loc[['BHATNAGAR SANJAY']].shift(1, axis =1).fillna(0)\n",
    "\n",
    "df.update(test_df)\n",
    "\n",
    "if not (df[df[payments].sum(axis = 1) != df.total_payments].shape[0] | df[df[stock_value].sum(\n",
    "    axis='columns') != df.total_stock_value][financial_feat_list].shape[0]):\n",
    "    print(\"All the financial data has been corrected\")\n",
    "else:\n",
    "    print(\"Some errors remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE NEW FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most straightforward manner of creating new features, in this case, is by using the existing ones. For example, we can create meaningful ratios of two features. A more complicated way to achieve the same goal is to work extensively with the full Enron email dataset. As we were curious about those cases of existing emails and no email related data, we decided to dive into the Enron email data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the mysterious missing data\n",
    "\n",
    "Exploring the Enron email dataset proved to be a time-consuming task. After searching with an intricate pattern of regular expressions and using specific search criteria based on the observed email addresses patterns, we were able to find up to 424 different email addresses linked to the people under study. Our search methods were far from optimal as they included final manual adjudications in many cases. That is why we have reasons to believe that there could be more email addresses than the ones we were able to find (but we decided to leave that as a subject of a more detailed study to be carried out in the future). In any case, our search allowed us to find some of the missing email addresses, and with that information, we built the email-based existing features for the employees including those \"strange 25 cases\". The code is too large to be inserted here, but we provide a text file with the procedure followed alongside with the script files we used. We are going to load a dictionary we created, similar to data_dict in structure, but with the data we processed directly from the Enron email dataset. It also contains new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_cc_this_person</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FASTOW ANDREW S</th>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>1136</td>\n",
       "      <td>1183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERBERIAN DAVID</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHRISTODOULOU DIOMEDES</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>448</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAGER F SCOTT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STABLER FRANK</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRENTICE JAMES</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OVERDYKE JR JERE C</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>374</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECHOLS JOHN B</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WODRASKA JOHN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KISHKILL JOSEPH G</th>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>274</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOLD JOSEPH</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>891</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HIRKO JOSEPH</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MORDAUNT KRISTINA M</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAI LOU L</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>542</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCRIMSHAW MATTHEW</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>431</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KOPPER MICHAEL J</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIMICHELE RICHARD G</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WESTFAHL RICHARD K</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUTTS ROBERT H</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HERMANN ROBERT J</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>194</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELLIOTT STEVEN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHITE JR THOMAS E</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>601</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETMERING TIMOTHY J</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>174</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LINDHOLM TOD A</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        from_messages  from_poi_cc_this_person  \\\n",
       "index                                                            \n",
       "FASTOW ANDREW S                     9                       15   \n",
       "BERBERIAN DAVID                     1                        1   \n",
       "CHRISTODOULOU DIOMEDES              1                        0   \n",
       "YEAGER F SCOTT                      0                        0   \n",
       "STABLER FRANK                       0                        0   \n",
       "BAY FRANKLIN R                      1                        0   \n",
       "PRENTICE JAMES                      6                        0   \n",
       "OVERDYKE JR JERE C                  3                        0   \n",
       "ECHOLS JOHN B                       8                        0   \n",
       "WODRASKA JOHN                       0                        0   \n",
       "KISHKILL JOSEPH G                  19                        4   \n",
       "GOLD JOSEPH                         6                        3   \n",
       "HIRKO JOSEPH                        3                        0   \n",
       "MORDAUNT KRISTINA M                 6                        0   \n",
       "PAI LOU L                           0                        9   \n",
       "SCRIMSHAW MATTHEW                   8                        3   \n",
       "KOPPER MICHAEL J                    0                        3   \n",
       "DIMICHELE RICHARD G                 8                        0   \n",
       "WESTFAHL RICHARD K                  2                        3   \n",
       "BUTTS ROBERT H                      5                        3   \n",
       "HERMANN ROBERT J                    3                        0   \n",
       "ELLIOTT STEVEN                      0                        0   \n",
       "WHITE JR THOMAS E                   0                        0   \n",
       "DETMERING TIMOTHY J                13                       11   \n",
       "LINDHOLM TOD A                     18                        0   \n",
       "\n",
       "                        from_poi_to_this_person  from_this_person_to_poi  \\\n",
       "index                                                                      \n",
       "FASTOW ANDREW S                              49                        5   \n",
       "BERBERIAN DAVID                               5                        0   \n",
       "CHRISTODOULOU DIOMEDES                        0                        1   \n",
       "YEAGER F SCOTT                               10                        0   \n",
       "STABLER FRANK                                 4                        0   \n",
       "BAY FRANKLIN R                               10                        0   \n",
       "PRENTICE JAMES                                8                        2   \n",
       "OVERDYKE JR JERE C                           33                        0   \n",
       "ECHOLS JOHN B                                 8                        5   \n",
       "WODRASKA JOHN                                 0                        0   \n",
       "KISHKILL JOSEPH G                            52                        2   \n",
       "GOLD JOSEPH                                  17                        0   \n",
       "HIRKO JOSEPH                                  4                        3   \n",
       "MORDAUNT KRISTINA M                           7                        3   \n",
       "PAI LOU L                                    25                        0   \n",
       "SCRIMSHAW MATTHEW                            46                        4   \n",
       "KOPPER MICHAEL J                             11                        0   \n",
       "DIMICHELE RICHARD G                           8                        1   \n",
       "WESTFAHL RICHARD K                           36                        0   \n",
       "BUTTS ROBERT H                               18                        0   \n",
       "HERMANN ROBERT J                             14                        3   \n",
       "ELLIOTT STEVEN                                8                        0   \n",
       "WHITE JR THOMAS E                             0                        0   \n",
       "DETMERING TIMOTHY J                          30                        4   \n",
       "LINDHOLM TOD A                               17                        4   \n",
       "\n",
       "                        shared_receipt_with_poi  to_messages  \n",
       "index                                                         \n",
       "FASTOW ANDREW S                            1136         1183  \n",
       "BERBERIAN DAVID                             158          159  \n",
       "CHRISTODOULOU DIOMEDES                      448          521  \n",
       "YEAGER F SCOTT                               81           88  \n",
       "STABLER FRANK                                36           89  \n",
       "BAY FRANKLIN R                               47          124  \n",
       "PRENTICE JAMES                               72          344  \n",
       "OVERDYKE JR JERE C                          374          465  \n",
       "ECHOLS JOHN B                                78           90  \n",
       "WODRASKA JOHN                                13           96  \n",
       "KISHKILL JOSEPH G                           274          392  \n",
       "GOLD JOSEPH                                 891         1060  \n",
       "HIRKO JOSEPH                                104          106  \n",
       "MORDAUNT KRISTINA M                         121          403  \n",
       "PAI LOU L                                   542          544  \n",
       "SCRIMSHAW MATTHEW                           431          518  \n",
       "KOPPER MICHAEL J                            183          192  \n",
       "DIMICHELE RICHARD G                         154          256  \n",
       "WESTFAHL RICHARD K                           21           64  \n",
       "BUTTS ROBERT H                              199          283  \n",
       "HERMANN ROBERT J                            194          174  \n",
       "ELLIOTT STEVEN                               87          194  \n",
       "WHITE JR THOMAS E                           601          623  \n",
       "DETMERING TIMOTHY J                         174          272  \n",
       "LINDHOLM TOD A                              170          260  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"missing_data_df.pickle\", \"r\") as data_file:\n",
    "    missing_data_df = pickle.load(data_file)\n",
    "\n",
    "missing_data_df = missing_data_df[missing_data_df.index.isin(strange_cases)]\n",
    "missing_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mystery solved: We were able to find the email-related data belonging to those 25 people with valid email addresses. Taking a quick look at the data we realized that there is a disproportion between the number of emails sent and received. The amount of messages sent by these people is suspiciously low (or inexistent) for the timeframe considered. We found email data for additional 19 people (from our second list of 35 shown above) that displays the same trend. \n",
    "\n",
    "One particular case in the above data frame is worth noticing: Andrew S. Fastow. It is hard to believe that the chief financial officer of a corporation, (who received at least 1183 emails) just sent nine emails in more than a year, including the time when the financial scandal shattered the company.\n",
    "\n",
    "I believe that the process of emails removal from the dataset due to privacy protection issues that occurred at some point after the first release of the Enron email data might have something to do with this. As this might be an intentional intervention in the data set, it definitively could affect the outcome of any attempt of classification if these data were to be included. It is, therefore, reasonable to assume that this particular situation is the reason behind the absence of the email-related features for those 25 \"strange cases\" we found earlier. \n",
    "\n",
    "### New features\n",
    "\n",
    "Having the Enron email dataset in a workable shape makes possible to create any number of new features. In this study we are going to try some that are very easy to create. We proposed four new features, belonging to two different kinds. Two were ratios of the existing features as we mentioned earlier, and the other two were the result of working with the entire email dataset. \n",
    "In the second case, we created an intermediate feature, called pubIndex.  This one is not going to be used explicitly, but it is part of the process.\n",
    "\n",
    " pubIndex accounts for the number of people involved in a given email (To and Cc fields) correcting for when people sent emails to themselves. The lowest possible value for this feature is zero (if someone sent an email just to him or herself with no Cc), it is equal to one if there is only a single person in the To field and none in the Cc field, and so on. It is worth noticing that there is in principle no upper limit for this feature.  \n",
    "\n",
    "### Ratios of existing features:\n",
    "\n",
    "- to_poi_rate: ratio of from_this_person_to_poi / from_messages\n",
    "\n",
    "- from_poi_rate: ratio of from_poi_to_this_person / to_messages\n",
    "\n",
    "### New features from the email dataset:\n",
    "\n",
    "- from_messages_median_pubIndex: We grouped all the emails sent by a given person and took the median of the pubIndex feature.\n",
    "\n",
    "- to_poi_median_pubIndex: The same as above but considering just when sending messages to poi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_data_df.pickle\", \"r\") as data_file:\n",
    "    df_new = pickle.load(data_file)\n",
    "    df_new.drop(['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['to_poi_rate'] = df['from_this_person_to_poi']/df['from_messages']\n",
    "df['from_poi_rate'] = df['from_poi_to_this_person']/df['to_messages']\n",
    "new_feat_list = ['from_messages_median_pubIndex', 'to_poi_median_pubIndex']\n",
    "df = pd.concat([df, df_new], axis=1)\n",
    "df[new_feat_list]=df[new_feat_list].fillna(df.groupby(\"poi\")[new_feat_list].transform(\"median\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of email-related missing data:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of email-related missing data: \", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features:  23\n"
     ]
    }
   ],
   "source": [
    "features_list = poi_label + financial_feat_list + email_feat_list + ['to_poi_rate', 'from_poi_rate' ] + new_feat_list\n",
    "print(\"Total number of features: \", len(features_list)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = df.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHM TRAINING\n",
    "\n",
    "### Extract features and labels from dataset for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, \n",
    "                                                                            test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for parameter tuning in grid search \n",
    "sss = StratifiedShuffleSplit(n_splits = 100, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best score:  0.33 \n",
      "\n",
      "\n",
      " Optimum number of features, KBest:  9 \n",
      "\n",
      "bonus ===> 30.73\n",
      "to_poi_rate ===> 23.94\n",
      "shared_receipt_with_poi ===> 16.97\n",
      "salary ===> 15.86\n",
      "total_stock_value ===> 10.63\n",
      "to_poi_median_pubIndex ===> 10.53\n",
      "exercised_stock_options ===> 9.68\n",
      "total_payments ===> 8.96\n",
      "deferred_income ===> 8.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "select = SelectKBest()\n",
    "clf = GaussianNB()\n",
    "\n",
    "steps = [\n",
    "\t\t # Preprocessing\n",
    "         ('standard_scaler', scaler),\n",
    "         \n",
    "         # Feature selection\n",
    "         ('feature_selection', select),\n",
    "         \n",
    "         # Classifier\n",
    "         ('clf', clf)\n",
    "         ]\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "parameters = dict(feature_selection__k=[2, 3, 5, 6, 7, 8, 9, 10, 12])\n",
    "\n",
    "\n",
    "# Create, fit, and make predictions with grid search\n",
    "gs = GridSearchCV(pipeline,\n",
    "                  param_grid=parameters,\n",
    "                  scoring=\"recall\",\n",
    "                  cv=sss.split(features_train, labels_train),\n",
    "                  error_score=0)\n",
    "gs.fit(features_train, labels_train)\n",
    "\n",
    "labels_predictions = gs.predict(features_test)\n",
    "\n",
    "\n",
    "print(\" Best score: \", gs.best_score_ , \"\\n\")\n",
    "\n",
    "classif_report = classification_report(labels_test, labels_predictions)\n",
    "\n",
    "scores = gs.best_estimator_.named_steps['feature_selection'].scores_\n",
    "mask = gs.best_estimator_.named_steps['feature_selection'].get_support()\n",
    "\n",
    "kselect_features = [] \n",
    "feat_importance = []\n",
    "for bool, feature, score in zip(mask, features_list[1:], scores):\n",
    "    if bool:\n",
    "        kselect_features.append(feature)\n",
    "        feat_importance.append([feature, round(score, 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "print (\"\\n\", \"Optimum number of features, KBest: \", gs.best_params_['feature_selection__k'], \"\\n\")\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "\n",
    "kselect_features.insert(0, \"poi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('standard_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('feature_selection', SelectKBest(k=9, score_func=<function f_classif at 0x1092e37d0>)), ('clf', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.86453\tPrecision: 0.48938\tRecall: 0.36850\tF1: 0.42042\tF2: 0.38765\n",
      "\tTotal predictions: 15000\tTrue positives:  737\tFalse positives:  769\tFalse negatives: 1263\tTrue negatives: 12231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_pipe = Pipeline([('standard_scaler', StandardScaler()),\n",
    "                          ('feature_selection', SelectKBest(k=gs.best_params_['feature_selection__k'])),\n",
    "                          ('clf', GaussianNB())])\n",
    "\n",
    "tester.dump_classifier_and_data(best_pipe, my_dataset, kselect_features)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant features, Decision Tree:\n",
      "to_poi_rate ===> 0.41\n",
      "shared_receipt_with_poi ===> 0.3\n",
      "to_messages ===> 0.13\n",
      "from_poi_to_this_person ===> 0.09\n",
      "total_payments ===> 0.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        feat_importance.append([df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "print('Most relevant features, Decision Tree:')\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "tree_feat_list = [x[0] for x in feat_importance]\n",
    "tree_feat_list.insert(0, 'poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data = featureFormat(my_dataset, tree_feat_list, sort_keys = True)\n",
    "t_labels, t_features = targetFeatureSplit(tree_data)\n",
    "t_features_train, t_features_test, t_labels_train, t_labels_test = train_test_split(t_features, t_labels, \n",
    "                                                                            test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree, best parameters set: \n",
      "splitter ==> best\n",
      "random_state ==> 45\n",
      "min_samples_leaf ==> 1\n",
      "min_weight_fraction_leaf ==> 0\n",
      "criterion ==> gini\n",
      "min_samples_split ==> 2\n",
      "max_depth ==> 2\n",
      "class_weight ==> balanced\n"
     ]
    }
   ],
   "source": [
    "parameters = dict(\n",
    "                  criterion=['gini', 'entropy'],\n",
    "                  splitter=['best', 'random'],\n",
    "                  max_depth=[None, 1, 2, 3, 4],\n",
    "                  min_samples_split=[2, 3, 4, 5],\n",
    "                  min_samples_leaf=[1, 2, 3, 4],\n",
    "                  min_weight_fraction_leaf=[0, 0.25, 0.5],\n",
    "                  class_weight=[None, 'balanced'],\n",
    "                  random_state=[45], \n",
    "                  )\n",
    "\n",
    "dt_clf = GridSearchCV(DecisionTreeClassifier(random_state = 45), param_grid = parameters, cv=sss.split(\n",
    "                                          features_train, labels_train),scoring='f1')\n",
    "dt_clf.fit(t_features_train, t_labels_train)\n",
    "t_labels_predictions = dt_clf.predict(t_features_test)\n",
    "classif_report = classification_report(t_labels_test, t_labels_predictions)\n",
    "print(\"Decision Tree, best parameters set: \")\n",
    "for key in dt_clf.best_params_:\n",
    "    print(key, \"==>\", dt_clf.best_params_[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0, presort=False, random_state=45,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.90707\tPrecision: 0.61173\tRecall: 0.82950\tF1: 0.70416\tF2: 0.77437\n",
      "\tTotal predictions: 15000\tTrue positives: 1659\tFalse positives: 1053\tFalse negatives:  341\tTrue negatives: 11947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(class_weight = dt_clf.best_params_['class_weight'], \n",
    "                             criterion = dt_clf.best_params_['criterion'], \n",
    "                             max_depth = dt_clf.best_params_['max_depth'], \n",
    "                             min_samples_leaf = dt_clf.best_params_['min_samples_leaf'], \n",
    "                             min_samples_split = dt_clf.best_params_['min_samples_split'], \n",
    "                             min_weight_fraction_leaf = dt_clf.best_params_['min_weight_fraction_leaf'],  \n",
    "                             splitter = dt_clf.best_params_['splitter'], \n",
    "                             random_state = 45)\n",
    "dump_classifier_and_data(clf, my_dataset, tree_feat_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant features, AdaBoost:\n",
      "expenses ===> 0.14\n",
      "shared_receipt_with_poi ===> 0.12\n",
      "to_poi_rate ===> 0.12\n",
      "from_poi_rate ===> 0.12\n",
      "from_this_person_to_poi ===> 0.1\n",
      "long_term_incentive ===> 0.08\n",
      "to_poi_median_pubIndex ===> 0.08\n",
      "other ===> 0.06\n",
      "salary ===> 0.04\n",
      "exercised_stock_options ===> 0.04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(random_state = 45)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.02:\n",
    "        feat_importance.append([df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "print(\"Most relevant features, AdaBoost:\")\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "boost_feat_list = [x[0] for x in feat_importance]\n",
    "boost_feat_list.insert(0, 'poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_data = featureFormat(my_dataset, boost_feat_list, sort_keys = True)\n",
    "b_labels, b_features = targetFeatureSplit(boost_data)\n",
    "b_features_train, b_features_test, b_labels_train, b_labels_test = train_test_split(b_features, b_labels, \n",
    "                                                                            test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost, best parameters set:\n",
      "n_estimators ==> 50\n",
      "learning_rate ==> 0.001\n"
     ]
    }
   ],
   "source": [
    "param_grid = {              \n",
    "              \"n_estimators\": [50, 100, 200, 400, 600, 800],\n",
    "              \"learning_rate\": [0.001, 0.01, 0.1, 1]\n",
    "             }\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state = 45)\n",
    "ABC = AdaBoostClassifier(base_estimator = DTC, random_state = 45)\n",
    "\n",
    "boost_clf = GridSearchCV(ABC, param_grid=param_grid, scoring = 'f1')\n",
    "\n",
    "boost_clf.fit(b_features_train, b_labels_train)\n",
    "b_labels_predictions = boost_clf.predict(b_features_test)\n",
    "b_classif_report = classification_report(b_labels_test, b_labels_predictions)\n",
    "\n",
    "print(\"AdaBoost, best parameters set:\")\n",
    "for key in boost_clf.best_params_:\n",
    "    print(key, \"==>\", boost_clf.best_params_[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized base estimator parameters set:\n",
      "base_estimator__criterion ==> entropy\n",
      "base_estimator__max_depth ==> 2\n",
      "base_estimator__min_samples_leaf ==> 1\n",
      "base_estimator__min_samples_split ==> 2\n",
      "base_estimator__splitter ==> best\n"
     ]
    }
   ],
   "source": [
    "param_grid_2 = {\n",
    "              \"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :  [\"best\", \"random\"],\n",
    "              \"base_estimator__max_depth\" : [None, 1, 2, 3, 4],\n",
    "              \"base_estimator__min_samples_leaf\" : [1, 2, 3, 4], \n",
    "              \"base_estimator__min_samples_split\" : [2, 3, 4, 5, 6]\n",
    "             }\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state = 45, class_weight = None, min_weight_fraction_leaf = 0)\n",
    "ABC = AdaBoostClassifier(base_estimator = DTC, \n",
    "                         random_state = 45,\n",
    "                         n_estimators = boost_clf.best_params_['n_estimators'], \n",
    "                         learning_rate = boost_clf.best_params_['learning_rate'],\n",
    "                        )\n",
    "\n",
    "# run grid search\n",
    "boost_clf_2 = GridSearchCV(ABC, param_grid=param_grid_2, scoring = 'f1')\n",
    "\n",
    "boost_clf_2.fit(b_features_train, b_labels_train)\n",
    "print(\"Optimized base estimator parameters set:\")\n",
    "for key in boost_clf_2.best_params_:\n",
    "    print(key, \"==>\", boost_clf_2.best_params_[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0, presort=False, random_state=45,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.001, n_estimators=50, random_state=45)\n",
      "\tAccuracy: 0.92307\tPrecision: 0.67393\tRecall: 0.81950\tF1: 0.73962\tF2: 0.78556\n",
      "\tTotal predictions: 15000\tTrue positives: 1639\tFalse positives:  793\tFalse negatives:  361\tTrue negatives: 12207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier(random_state = 45, \n",
    "                             class_weight = None, \n",
    "                             min_weight_fraction_leaf = 0, \n",
    "                             criterion = boost_clf_2.best_params_['base_estimator__criterion'], \n",
    "                             max_depth = boost_clf_2.best_params_['base_estimator__max_depth'],\n",
    "                             splitter =  boost_clf_2.best_params_['base_estimator__splitter'], \n",
    "                             min_samples_leaf = boost_clf_2.best_params_['base_estimator__min_samples_leaf'],\n",
    "                             min_samples_split = boost_clf_2.best_params_['base_estimator__min_samples_split']\n",
    "                            )\n",
    "clf = AdaBoostClassifier(base_estimator = DTC, learning_rate = boost_clf.best_params_['learning_rate'], \n",
    "                         n_estimators = boost_clf.best_params_['n_estimators'], random_state = 45)\n",
    "tester.dump_classifier_and_data(clf, my_dataset, boost_feat_list)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tunning our three classifiers, we obtained in general decent values for all the relevant metrics (as all of them comply with the minimum requirement of being above 0.3). It is worth mentioning that we tried this out using a fixed random state for the sake of reproducibility. If we remove this restriction,  the results obtained after running tester.py will change from one run to the next. However, as an average, we expect them to be close to those shown in the table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.864</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.907</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Precision  Recall     F1\n",
       "GaussianNB        0.864      0.489   0.368  0.420\n",
       "Decision Tree     0.907      0.612   0.829  0.704\n",
       "AdaBoost          0.923      0.674   0.819  0.739"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Final Results\")\n",
    "pd.DataFrame([[0.864, 0.489, 0.368, 0.420], [0.907, 0.612, 0.829, 0.704],[0.923, 0.674, 0.819, 0.739]],\n",
    "             columns = ['Accuracy','Precision', 'Recall', 'F1'], \n",
    "             index = ['GaussianNB', 'Decision Tree', 'AdaBoost'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS\n",
    "\n",
    "We applied three well-known machine learning algorithms to a combination of financial data (FindLaw) and email-related data from the Enron dataset in an attempt to find persons of interest (poi) in the Enron scandal case.\n",
    "As it is almost always the case in Data Analysis, the preprocessing of the data played an essential role in the development and final results of our project.\n",
    "\n",
    "The data set provided contained information about 144 people involved (18 of them were poi); which is, by all means, a small amount of data for the intended task. With an initial assessment of 1323 missing entries within which there were 25 \"strange cases\" of missing email information, the prospects for success were not precisely enjoyable. Fortunately, we learned that the missing (NaN) values in the financial data were in reality zeros and that dramatically reduced our missing entries to a more manageable amount of 285.  After that, and making use of the Enron email dataset, we were able to unravel the mystery of those 25 missing cases and decided in correspondence. We chose to impute the NaN values with the median of their respective columns making the difference between poi and non-poi. That decision was not taken blindly. It was such that it maximized the performance of our classifiers.\n",
    "\n",
    "After applying Gaussian Naive Bayes, Decision Tree and AdaBoost classifiers to a reduced number of features, we observed that all three of them fulfilled the minimum requirements for the project. Decision Tree and AdaBoost were very close in their performances. We decided to concede the edge to AdaBoost, not because of its marginally better numbers but taking into account that by using Decision Tree inside boosting algorithms, we make it more robust when facing new data.  We tuned AdaBoost using GridSearchCV in two consecutive steps to minimize the running time. In the end, after applying tester.py, our best values (for a definite random state) were as follows: Accuracy: 0.923, Precision: 0.674, Recall 0.819 and F1 0.739. \n",
    "We believe that this set of values is indicative of a solid performance by that classifier in this problem, but there is still room for further improvements. \n",
    "\n",
    "\n",
    "## Limitations and future work\n",
    "\n",
    "As we mentioned above, one of the inherent limitations of this project emanates from the small size of the data set, just 144 people (18 poi). The quality of the data also played a significant role, as the government intervention for privacy issues substantially modified part of the data making them useless for this study.\n",
    "What is exciting about this project is that by using the full email data set, it is possible to create, at least in principle, any number of new and meaningful features. I believe that if we work this process in more detail,  we could end up creating features that could improve further the efficiency of our classifiers. For instance, some features could take into account the flux of emails around the critical dates, or be the result of sentiment analysis and clustering applied to the emails texts.\n",
    "\n",
    "\n",
    "# REFERENCES\n",
    "\n",
    "1. http://www.ahschulz.de/enron-email-data/\n",
    "2. https://enrondata.readthedocs.io/en/latest/data/custodian-names-and-titles/\n",
    "3. http://www.infosys.tuwien.ac.at/staff/dschall/email/enron-employees.txt\n",
    "4. https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/\n",
    "5. https://regex101.com\n",
    "8. https://rodgersnotes.wordpress.com/2013/11/19/enron-email-analysis-persons-of-interest/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
