{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../tools/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.decomposition import PCA\n",
    "import tester\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"rb\"))\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that all the email-related data were collected using valid email addresses, therefore if some people have their email addresses missing, that implies the corresponding email features will be missing as well (NaN). The contrary is not true as we are about to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with missing email data:  60\n",
      "Number of people with missing email address:  35\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of people with missing email data: \",df['from_messages'].value_counts().max())\n",
    "print(\"Number of people with missing email address: \",df['email_address'].value_counts().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are these people?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People with at least one email address but without email-related features: \n",
      "\n",
      "1 ELLIOTT STEVEN\n",
      "2 MORDAUNT KRISTINA M\n",
      "3 WESTFAHL RICHARD K\n",
      "4 WODRASKA JOHN\n",
      "5 ECHOLS JOHN B\n",
      "6 KOPPER MICHAEL J\n",
      "7 BERBERIAN DAVID\n",
      "8 DETMERING TIMOTHY J\n",
      "9 GOLD JOSEPH\n",
      "10 KISHKILL JOSEPH G\n",
      "11 LINDHOLM TOD A\n",
      "12 BUTTS ROBERT H\n",
      "13 HERMANN ROBERT J\n",
      "14 SCRIMSHAW MATTHEW\n",
      "15 FASTOW ANDREW S\n",
      "16 OVERDYKE JR JERE C\n",
      "17 STABLER FRANK\n",
      "18 PRENTICE JAMES\n",
      "19 WHITE JR THOMAS E\n",
      "20 CHRISTODOULOU DIOMEDES\n",
      "21 DIMICHELE RICHARD G\n",
      "22 YEAGER F SCOTT\n",
      "23 HIRKO JOSEPH\n",
      "24 PAI LOU L\n",
      "25 BAY FRANKLIN R\n"
     ]
    }
   ],
   "source": [
    "strange_cases = df.index[(df['email_address'] != 'NaN') & (df['from_messages']=='NaN')].tolist()\n",
    "print(\"People with at least one email address but without email-related features: \")\n",
    "print()\n",
    "for i, name in enumerate(strange_cases):\n",
    "    print(i + 1, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are 25 extrange cases, where actual Enron employees with a valid email address,  do not have email-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People without an email address: \n",
      "\n",
      "1 BAXTER JOHN C\n",
      "2 LOWRY CHARLES P\n",
      "3 WALTERS GARETH W\n",
      "4 CHAN RONNIE\n",
      "5 BELFER ROBERT\n",
      "6 URQUHART JOHN A\n",
      "7 WHALEY DAVID A\n",
      "8 MENDELSOHN JOHN\n",
      "9 CLINE KENNETH W\n",
      "10 WAKEHAM JOHN\n",
      "11 DUNCAN JOHN H\n",
      "12 LEMAISTRE CHARLES\n",
      "13 SULLIVAN-SHAKLOVITZ COLLEEN\n",
      "14 WROBEL BRUCE\n",
      "15 MEYER JEROME J\n",
      "16 CUMBERLAND MICHAEL S\n",
      "17 GAHN ROBERT S\n",
      "18 GATHMANN WILLIAM D\n",
      "19 GILLIS JOHN\n",
      "20 BAZELIDES PHILIP J\n",
      "21 LOCKHART EUGENE E\n",
      "22 PEREIRA PAULO V. FERRAZ\n",
      "23 BLAKE JR. NORMAN P\n",
      "24 GRAY RODNEY\n",
      "25 THE TRAVEL AGENCY IN THE PARK\n",
      "26 NOLES JAMES L\n",
      "27 TOTAL\n",
      "28 JAEDICKE ROBERT\n",
      "29 WINOKUR JR. HERBERT S\n",
      "30 BADUM JAMES P\n",
      "31 REYNOLDS LAWRENCE\n",
      "32 YEAP SOON\n",
      "33 FUGH JOHN L\n",
      "34 SAVAGE FRANK\n",
      "35 GRAMM WENDY L\n"
     ]
    }
   ],
   "source": [
    "emailless_people = df.index[df['email_address'] == 'NaN'].tolist()\n",
    "print(\"People without an email address: \")\n",
    "print()\n",
    "for i, name in enumerate(emailless_people):\n",
    "    print(i + 1, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we found two entities, that are not real people: THE TRAVEL AGENCY IN THE PARK and TOTAL. These entities do not contribute in any meaningful way to the purpose of this study, so let say that from this moment on we mark them for deletion. \n",
    "\n",
    "The email address is the only field that could not be converted to numeric. We chose to remove it from the data frame because it is of no use to identify poi from the data given.\n",
    "Also, in the case of the poi column only zeroes (0) and ones (1) are allowed: 1 = poi, 0 = non-poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.apply(lambda x: pd.to_numeric(x, errors='coerse'))\n",
    "del df['email_address']\n",
    "df['poi']=df['poi'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, METTS MARK to GLISAN JR BEN F\n",
      "Data columns (total 20 columns):\n",
      "poi                          146 non-null int64\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "shared_receipt_with_poi      86 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "dtypes: float64(19), int64(1)\n",
      "memory usage: 24.0+ KB\n"
     ]
    }
   ],
   "source": [
    "poi_label = ['poi']\n",
    "financial_feat_list = ['salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments', \n",
    "                       'loan_advances', 'other', 'expenses', 'director_fees', 'total_payments', \n",
    "                       'exercised_stock_options', 'restricted_stock','restricted_stock_deferred', 'total_stock_value']\n",
    "email_feat_list = ['from_messages', 'from_poi_to_this_person','from_this_person_to_poi', 'shared_receipt_with_poi', \n",
    "                   'to_messages']\n",
    "features_list = poi_label + financial_feat_list + email_feat_list\n",
    "df=df[features_list]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of poi and non-poi in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    128\n",
       "1     18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of poi without email data:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of poi without email data: \", df[(df['poi']==1) & (~df.to_messages.notnull())].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are people without data associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOCKHART EUGENE E</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "LOCKHART EUGENE E    0     NaN    NaN                  NaN              NaN   \n",
       "\n",
       "                   deferral_payments  loan_advances  other  expenses  \\\n",
       "LOCKHART EUGENE E                NaN            NaN    NaN       NaN   \n",
       "\n",
       "                   director_fees  total_payments  exercised_stock_options  \\\n",
       "LOCKHART EUGENE E            NaN             NaN                      NaN   \n",
       "\n",
       "                   restricted_stock  restricted_stock_deferred  \\\n",
       "LOCKHART EUGENE E               NaN                        NaN   \n",
       "\n",
       "                   total_stock_value  from_messages  from_poi_to_this_person  \\\n",
       "LOCKHART EUGENE E                NaN            NaN                      NaN   \n",
       "\n",
       "                   from_this_person_to_poi  shared_receipt_with_poi  \\\n",
       "LOCKHART EUGENE E                      NaN                      NaN   \n",
       "\n",
       "                   to_messages  \n",
       "LOCKHART EUGENE E          NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().sum(axis=1) >= df.shape[1]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the only thing we know about Eugene E. Lockhart is that he is not a person of interest.\n",
    "\n",
    "As we have a relatively low number of data points, we are going to proceed extra-carefully at removing them.\n",
    "For the moment, we are going to do it just to the items we previously have marked for deletion plus this last one, and we will analyze any further need in a case by case manner as we proceed with our ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['TOTAL','THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the financial data, we learned that NaN means zero. Therefore we proceed to make the corresponding changes in our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 1:15] = df.iloc[:, 1:15].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing such an operation, the number of NaN values was dramatically reduced from 1323 up to 285, which ultimately is the amount of missing email-related entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of email-related missing data:  285\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of email-related missing data: \", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the best way to proceed with the remaining NaN values is to impute them with the median for non-poi people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of remaining NaN entries in the dataframe: 0\n"
     ]
    }
   ],
   "source": [
    "df[email_feat_list]=df[email_feat_list].fillna(df.groupby(\"poi\")[email_feat_list].transform(\"median\"))\n",
    "print(\"Amount of remaining NaN entries in the dataframe:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As found in some of our references, the manual input of the financial data could have been the cause of some observed mistakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT        0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  \n",
       "BELFER ROBERT              -44093.0  \n",
       "BHATNAGAR SANJAY                0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payments = financial_feat_list[:9]\n",
    "df[df[payments].sum(axis = 1) != df.total_payments][financial_feat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT        0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  \n",
       "BELFER ROBERT              -44093.0  \n",
       "BHATNAGAR SANJAY                0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_value = financial_feat_list[10:13]\n",
    "test_df=df[df[stock_value].sum(axis='columns') != df.total_stock_value][financial_feat_list]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there are errors in just two rows. Checking the .pdf document obtained from FindLaw, we acknowledged that the errors are in fact shifts of one column in each case but opposite directions. Let's correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the financial data has been corrected\n"
     ]
    }
   ],
   "source": [
    "test_df.loc[['BELFER ROBERT']] = test_df.loc[['BELFER ROBERT']].shift(-1, axis =1).fillna(0)\n",
    "test_df.loc[['BHATNAGAR SANJAY']] = test_df.loc[['BHATNAGAR SANJAY']].shift(1, axis =1).fillna(0)\n",
    "\n",
    "df.update(test_df)\n",
    "\n",
    "if not (df[df[payments].sum(axis = 1) != df.total_payments].shape[0] | df[df[stock_value].sum(\n",
    "    axis='columns') != df.total_stock_value][financial_feat_list].shape[0]):\n",
    "    print(\"All the financial data has been corrected\")\n",
    "else:\n",
    "    print(\"Some errors remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE NEW FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most straightforward manner of creating new features, in this case, is by using the existing ones. For example, we can create meaningful ratios of two features. A more complicated way to achieve the same goal is to work extensively with the full Enron email dataset. As we were curious about those cases of existing emails and no email related data, we decided to dive into the Enron email data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the mysterious missing data\n",
    "\n",
    "Exploring the Enron email dataset proved to be a time-consuming task. After searching with an intricate pattern of regular expressions and using specific search criteria based on the observed email addresses patterns, we were able to find up to 424 different email addresses linked to the people under study. Our search methods were far from optimal as they included final manual adjudications in many cases. That is why we have reasons to believe that there could be more email addresses than the ones we were able to find (but we decided to leave that as a subject of a more detailed study to be carried out in the future). In any case, our search allowed us to find some of the missing email addresses, and with that information, we built the email-based existing features for the employees including those \"strange 25 cases\". The code is too large to be inserted here, but we provide a text file with the procedure followed alongside with the script files we used. We are going to load a dictionary we created, similar to data_dict in structure, but with the data we processed directly from the Enron email dataset. It also contains new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_cc_this_person</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FASTOW ANDREW S</th>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>1136</td>\n",
       "      <td>1183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERBERIAN DAVID</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHRISTODOULOU DIOMEDES</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>448</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAGER F SCOTT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STABLER FRANK</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRENTICE JAMES</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OVERDYKE JR JERE C</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>374</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECHOLS JOHN B</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WODRASKA JOHN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KISHKILL JOSEPH G</th>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>274</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOLD JOSEPH</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>891</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HIRKO JOSEPH</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MORDAUNT KRISTINA M</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAI LOU L</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>542</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCRIMSHAW MATTHEW</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>431</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KOPPER MICHAEL J</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIMICHELE RICHARD G</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WESTFAHL RICHARD K</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUTTS ROBERT H</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HERMANN ROBERT J</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>194</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELLIOTT STEVEN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHITE JR THOMAS E</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>601</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETMERING TIMOTHY J</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>174</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LINDHOLM TOD A</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        from_messages  from_poi_cc_this_person  \\\n",
       "index                                                            \n",
       "FASTOW ANDREW S                     9                       15   \n",
       "BERBERIAN DAVID                     1                        1   \n",
       "CHRISTODOULOU DIOMEDES              1                        0   \n",
       "YEAGER F SCOTT                      0                        0   \n",
       "STABLER FRANK                       0                        0   \n",
       "BAY FRANKLIN R                      1                        0   \n",
       "PRENTICE JAMES                      6                        0   \n",
       "OVERDYKE JR JERE C                  3                        0   \n",
       "ECHOLS JOHN B                       8                        0   \n",
       "WODRASKA JOHN                       0                        0   \n",
       "KISHKILL JOSEPH G                  19                        4   \n",
       "GOLD JOSEPH                         6                        3   \n",
       "HIRKO JOSEPH                        3                        0   \n",
       "MORDAUNT KRISTINA M                 6                        0   \n",
       "PAI LOU L                           0                        9   \n",
       "SCRIMSHAW MATTHEW                   8                        3   \n",
       "KOPPER MICHAEL J                    0                        3   \n",
       "DIMICHELE RICHARD G                 8                        0   \n",
       "WESTFAHL RICHARD K                  2                        3   \n",
       "BUTTS ROBERT H                      5                        3   \n",
       "HERMANN ROBERT J                    3                        0   \n",
       "ELLIOTT STEVEN                      0                        0   \n",
       "WHITE JR THOMAS E                   0                        0   \n",
       "DETMERING TIMOTHY J                13                       11   \n",
       "LINDHOLM TOD A                     18                        0   \n",
       "\n",
       "                        from_poi_to_this_person  from_this_person_to_poi  \\\n",
       "index                                                                      \n",
       "FASTOW ANDREW S                              49                        5   \n",
       "BERBERIAN DAVID                               5                        0   \n",
       "CHRISTODOULOU DIOMEDES                        0                        1   \n",
       "YEAGER F SCOTT                               10                        0   \n",
       "STABLER FRANK                                 4                        0   \n",
       "BAY FRANKLIN R                               10                        0   \n",
       "PRENTICE JAMES                                8                        2   \n",
       "OVERDYKE JR JERE C                           33                        0   \n",
       "ECHOLS JOHN B                                 8                        5   \n",
       "WODRASKA JOHN                                 0                        0   \n",
       "KISHKILL JOSEPH G                            52                        2   \n",
       "GOLD JOSEPH                                  17                        0   \n",
       "HIRKO JOSEPH                                  4                        3   \n",
       "MORDAUNT KRISTINA M                           7                        3   \n",
       "PAI LOU L                                    25                        0   \n",
       "SCRIMSHAW MATTHEW                            46                        4   \n",
       "KOPPER MICHAEL J                             11                        0   \n",
       "DIMICHELE RICHARD G                           8                        1   \n",
       "WESTFAHL RICHARD K                           36                        0   \n",
       "BUTTS ROBERT H                               18                        0   \n",
       "HERMANN ROBERT J                             14                        3   \n",
       "ELLIOTT STEVEN                                8                        0   \n",
       "WHITE JR THOMAS E                             0                        0   \n",
       "DETMERING TIMOTHY J                          30                        4   \n",
       "LINDHOLM TOD A                               17                        4   \n",
       "\n",
       "                        shared_receipt_with_poi  to_messages  \n",
       "index                                                         \n",
       "FASTOW ANDREW S                            1136         1183  \n",
       "BERBERIAN DAVID                             158          159  \n",
       "CHRISTODOULOU DIOMEDES                      448          521  \n",
       "YEAGER F SCOTT                               81           88  \n",
       "STABLER FRANK                                36           89  \n",
       "BAY FRANKLIN R                               47          124  \n",
       "PRENTICE JAMES                               72          344  \n",
       "OVERDYKE JR JERE C                          374          465  \n",
       "ECHOLS JOHN B                                78           90  \n",
       "WODRASKA JOHN                                13           96  \n",
       "KISHKILL JOSEPH G                           274          392  \n",
       "GOLD JOSEPH                                 891         1060  \n",
       "HIRKO JOSEPH                                104          106  \n",
       "MORDAUNT KRISTINA M                         121          403  \n",
       "PAI LOU L                                   542          544  \n",
       "SCRIMSHAW MATTHEW                           431          518  \n",
       "KOPPER MICHAEL J                            183          192  \n",
       "DIMICHELE RICHARD G                         154          256  \n",
       "WESTFAHL RICHARD K                           21           64  \n",
       "BUTTS ROBERT H                              199          283  \n",
       "HERMANN ROBERT J                            194          174  \n",
       "ELLIOTT STEVEN                               87          194  \n",
       "WHITE JR THOMAS E                           601          623  \n",
       "DETMERING TIMOTHY J                         174          272  \n",
       "LINDHOLM TOD A                              170          260  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"missing_data_df.pickle\", \"r\") as data_file:\n",
    "    missing_data_df = pickle.load(data_file)\n",
    "\n",
    "missing_data_df = missing_data_df[missing_data_df.index.isin(strange_cases)]\n",
    "missing_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mystery solved: We were able to find the email-related data belonging to those 25 people with valid email addresses. Taking a quick look at the data we realized that there is a disproportion between the number of emails sent and received. The amount of messages sent by these people is suspiciously low (or inexistent) for the timeframe considered. We found email data for additional 19 people (from our second list of 35 shown above) that displays the same trend. \n",
    "\n",
    "One particular case in the above data frame is worth noticing: Andrew S. Fastow. It is hard to believe that the chief financial officer of a corporation, (who received at least 1183 emails) just sent nine emails in more than a year, including the time when the financial scandal shattered the company.\n",
    "\n",
    "I believe that the process of emails removal from the dataset due to privacy protection issues that occurred at some point after the first release of the Enron email data might have something to do with this. As this might be an intentional intervention in the data set, it definitively could affect the outcome of any attempt of classification if these data were to be included. It is, therefore, reasonable to assume that this particular situation is the reason behind the absence of the email-related features for those 25 \"strange cases\" we found earlier. \n",
    "\n",
    "### New features\n",
    "\n",
    "Having the Enron email dataset in a workable shape makes possible to create any number of new features. In this study we are going to try some that are very easy to create. We proposed four new features, belonging to two different kinds. Two were ratios of the existing features as we mentioned earlier, and the other two were the result of working with the entire email dataset. \n",
    "In the second case, we created an intermediate feature, called pubIndex.  This one is not going to be used explicitly, but it is part of the process.\n",
    "\n",
    " pubIndex accounts for the number of people involved in a given email (To and Cc fields) correcting for when people sent emails to themselves. The lowest possible value for this feature is zero (if someone sent an email just to him or herself with no Cc), it is equal to one if there is only a single person in the To field and none in the Cc field, and so on. It is worth noticing that there is in principle no upper limit for this feature.  \n",
    "\n",
    "### Ratios of existing features:\n",
    "\n",
    "- to_poi_rate: ratio of from_this_person_to_poi / from_messages\n",
    "\n",
    "- from_poi_rate: ratio of from_poi_to_this_person / to_messages\n",
    "\n",
    "### New features from the email dataset:\n",
    "\n",
    "- from_messages_median_pubIndex: We grouped all the emails sent by a given person and took the median of the pubIndex feature.\n",
    "\n",
    "- to_poi_median_pubIndex: The same as above but considering just when sending messages to poi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_data_df.pickle\", \"r\") as data_file:\n",
    "    df_new = pickle.load(data_file)\n",
    "    df_new.drop(['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['to_poi_rate'] = df['from_this_person_to_poi']/df['from_messages']\n",
    "df['from_poi_rate'] = df['from_poi_to_this_person']/df['to_messages']\n",
    "new_feat_list = ['from_messages_median_pubIndex', 'to_poi_median_pubIndex']\n",
    "df = pd.concat([df, df_new], axis=1)\n",
    "df[new_feat_list]=df[new_feat_list].fillna(df.groupby(\"poi\")[new_feat_list].transform(\"median\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of email-related missing data:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of email-related missing data: \", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features:  23\n"
     ]
    }
   ],
   "source": [
    "features_list = poi_label + financial_feat_list + email_feat_list + ['to_poi_rate', 'from_poi_rate' ] + new_feat_list\n",
    "print(\"Total number of features: \", len(features_list)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = df.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL ALGORITHM TRAINING\n",
    "\n",
    "### Extract features and labels from dataset for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, \n",
    "                                                                            test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for parameter tuning in grid search \n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best parameters:  {'feature_selection__k': 2} \n",
      "\n",
      " Best score:  0.374074074074 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.89      0.89        38\n",
      "        1.0       0.20      0.20      0.20         5\n",
      "\n",
      "avg / total       0.81      0.81      0.81        43\n",
      "\n",
      "bonus ===> 30.73\n",
      "to_poi_rate ===> 23.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "select = SelectKBest()\n",
    "clf = GaussianNB()\n",
    "\n",
    "steps = [\n",
    "\t\t # Preprocessing\n",
    "         ('standard_scaler', scaler),\n",
    "         \n",
    "         # Feature selection\n",
    "         ('feature_selection', select),\n",
    "         \n",
    "         # Classifier\n",
    "         ('clf', clf)\n",
    "         ]\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "parameters = dict(feature_selection__k=[2, 3, 5, 6, 7, 8, 9, 10, 12])\n",
    "\n",
    "\n",
    "# Create, fit, and make predictions with grid search\n",
    "gs = GridSearchCV(pipeline,\n",
    "\t              param_grid=parameters,\n",
    "\t              scoring=\"f1\",\n",
    "\t              cv=sss.split(features_train, labels_train),\n",
    "\t              error_score=0)\n",
    "gs.fit(features_train, labels_train)\n",
    "\n",
    "labels_predictions = gs.predict(features_test)\n",
    "\n",
    "#clf = gs.best_estimator_\n",
    "\n",
    "print (\"\\n\", \"Best parameters: \", gs.best_params_, \"\\n\")\n",
    "print(\" Best score: \", gs.best_score_ , \"\\n\")\n",
    "\n",
    "classif_report = classification_report(labels_test, labels_predictions)\n",
    "print(classif_report)\n",
    "\n",
    "scores = gs.best_estimator_.named_steps['feature_selection'].scores_\n",
    "mask = gs.best_estimator_.named_steps['feature_selection'].get_support()\n",
    "\n",
    "kselect_features = [] \n",
    "feat_importance = []\n",
    "for bool, feature, score in zip(mask, features_list[1:], scores):\n",
    "    if bool:\n",
    "        kselect_features.append(feature)\n",
    "        feat_importance.append([feature, round(score, 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "\n",
    "kselect_features.insert(0, \"poi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('standard_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x1a10f669b0>)), ('clf', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.83967\tPrecision: 0.37809\tRecall: 0.31400\tF1: 0.34308\tF2: 0.32502\n",
      "\tTotal predictions: 15000\tTrue positives:  628\tFalse positives: 1033\tFalse negatives: 1372\tTrue negatives: 11967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tester.dump_classifier_and_data(pipeline, my_dataset, features_list)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('standard_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('feature_selection', SelectKBest(k=2, score_func=<function f_classif at 0x1a10f669b0>)), ('clf', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.83343\tPrecision: 0.34250\tRecall: 0.18050\tF1: 0.23641\tF2: 0.19936\n",
      "\tTotal predictions: 14000\tTrue positives:  361\tFalse positives:  693\tFalse negatives: 1639\tTrue negatives: 11307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_pipe = Pipeline([('standard_scaler', StandardScaler()),\n",
    "                          ('feature_selection', SelectKBest(k=2)),\n",
    "                          ('clf', GaussianNB())])\n",
    "\n",
    "tester.dump_classifier_and_data(best_pipe, my_dataset, kselect_features)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "select = SelectKBest()\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "steps = [\n",
    "\t\t # Preprocessing\n",
    "         ('standard_scaler', scaler),\n",
    "         \n",
    "         # Feature selection\n",
    "         ('feature_selection', select),\n",
    "         \n",
    "         # Classifier\n",
    "         ('dtc', dtc)\n",
    "         # ('svc', svc)\n",
    "         # ('knc', knc)\n",
    "         ]\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Parameters to try in grid search\n",
    "parameters = dict(\n",
    "                  feature_selection__k=[2, 3, 5, 6, 8, 10, 12], \n",
    "                  dtc__criterion=['gini', 'entropy'],\n",
    "                  dtc__splitter=['best', 'random'],\n",
    "                  dtc__max_depth=[None, 1, 2, 3, 4],\n",
    "                  dtc__min_samples_split=[2, 3, 4, 25],\n",
    "                  dtc__min_samples_leaf=[1, 2, 3, 4],\n",
    "                  dtc__min_weight_fraction_leaf=[0, 0.25, 0.5],\n",
    "                  dtc__class_weight=[None, 'balanced'],\n",
    "                  dtc__random_state=[45]\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best parameters are:  {'dtc__max_depth': None, 'dtc__criterion': 'gini', 'dtc__min_samples_leaf': 1, 'dtc__min_samples_split': 3, 'dtc__splitter': 'best', 'dtc__class_weight': None, 'feature_selection__k': 10, 'dtc__random_state': 45, 'dtc__min_weight_fraction_leaf': 0} \n",
      "\n",
      "\n",
      "It took 745.417464972 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# Create, fit, and make predictions with grid search\n",
    "gs = GridSearchCV(pipeline,\n",
    "                  param_grid=parameters,\n",
    "                  scoring=\"f1\",\n",
    "                  cv=sss.split(features_train, labels_train),\n",
    "                  error_score=0)\n",
    "gs.fit(features_train, labels_train)\n",
    "\n",
    "labels_predictions = gs.predict(features_test)\n",
    "\n",
    "#clf = gs.best_estimator_\n",
    "print (\"\\n\", \"Best parameters are: \", gs.best_params_, \"\\n\")\n",
    "print()\n",
    "print ('It took', time.time()-start, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best score:  0.85 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Best score: \", gs.best_score_ , \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  3,  9, 10, 11, 13, 17, 19, 22])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalFeatureIndices = gs.best_estimator_.named_steps['feature_selection'].get_support(indices=True)\n",
    "finalFeatureIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.066313  ,  0.02095606,  0.066313  ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.44112559,  0.40529236,  0.        ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_.named_steps['dtc'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('standard_scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x1a10f669b0>)), ('dtc', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.88887\tPrecision: 0.59858\tRecall: 0.50550\tF1: 0.54812\tF2: 0.52173\n",
      "\tTotal predictions: 15000\tTrue positives: 1011\tFalse positives:  678\tFalse negatives:  989\tTrue negatives: 12322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_pipe = Pipeline([('standard_scaler', StandardScaler()),\n",
    "                          ('feature_selection', SelectKBest(k=8)),\n",
    "                          ('clf', DecisionTreeClassifier(max_depth = 4, criterion = 'gini', \n",
    "                                                        min_samples_leaf = 1, min_samples_split = 2, \n",
    "                                                        splitter = 'best', class_weight = None, \n",
    "                                                        min_weight_fraction_leaf = 0, random_state = 45))])\n",
    "\n",
    "tester.dump_classifier_and_data(pipeline, my_dataset, features_list)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TESTING ZONE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "s_scaled = scaler.fit_transform(df.iloc[:,1:])\n",
    "scaled_df = pd.DataFrame(s_scaled, index=df.index)\n",
    "scaled_df.insert(0, \"poi\", df.poi)\n",
    "scaled_df.columns = features_list\n",
    "scaled_data_dict = scaled_df.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_poi_rate ===> 0.35\n",
      "shared_receipt_with_poi ===> 0.22\n",
      "expenses ===> 0.17\n",
      "other ===> 0.16\n",
      "from_poi_rate ===> 0.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.05:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "tree_feat_list = [x[0] for x in feat_importance]\n",
    "tree_feat_list.insert(0, 'poi')\n",
    "tree_lim_df = scaled_df[tree_feat_list]\n",
    "lim_feat_data_dict = tree_lim_df.to_dict(orient = 'index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = featureFormat(lim_feat_data_dict, tree_feat_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(sdata)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, \n",
    "                                                                            test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 3,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0,\n",
       " 'random_state': 45,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters to try in grid search\n",
    "parameters = dict(\n",
    "                  criterion=['gini', 'entropy'],\n",
    "                  splitter=['best', 'random'],\n",
    "                  max_depth=[None, 1, 2, 3, 4],\n",
    "                  min_samples_split=[2, 3, 4, 25],\n",
    "                  min_samples_leaf=[1, 2, 3, 4],\n",
    "                  min_weight_fraction_leaf=[0, 0.25, 0.5],\n",
    "                  class_weight=[None, 'balanced'],\n",
    "                  random_state=[45]\n",
    "                  )\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state = 45), param_grid = parameters, cv=sss.split(\n",
    "                                          features_train, labels_train),scoring='f1')\n",
    "clf.fit(features_train, labels_train)\n",
    "labels_predictions = clf.predict(features_test)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.92      0.95        38\n",
      "        1.0       0.57      0.80      0.67         5\n",
      "\n",
      "avg / total       0.93      0.91      0.91        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classif_report = classification_report(labels_test, labels_predictions)\n",
    "print(classif_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0, presort=False, random_state=45,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.93047\tPrecision: 0.70868\tRecall: 0.81250\tF1: 0.75705\tF2: 0.78937\n",
      "\tTotal predictions: 15000\tTrue positives: 1625\tFalse positives:  668\tFalse negatives:  375\tTrue negatives: 12332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(class_weight = None, criterion = 'entropy', max_depth = 3, \n",
    "                            min_samples_leaf = 3, min_samples_split = 2, min_weight_fraction_leaf = 0, \n",
    "                            random_state = 45, splitter = 'best')\n",
    "dump_classifier_and_data(clf, lim_feat_data_dict, tree_feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n_estimators',\n",
       " 'base_estimator',\n",
       " 'random_state',\n",
       " 'learning_rate',\n",
       " 'algorithm']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoostClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared_receipt_with_poi ===> 0.14\n",
      "to_poi_rate ===> 0.1\n",
      "expenses ===> 0.08\n",
      "from_this_person_to_poi ===> 0.08\n",
      "from_poi_rate ===> 0.08\n",
      "deferred_income ===> 0.06\n",
      "other ===> 0.06\n",
      "total_payments ===> 0.06\n",
      "exercised_stock_options ===> 0.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.04:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "feat_list = [x[0] for x in feat_importance]\n",
    "feat_list.insert(0, 'poi')\n",
    "lim_feat_sdata_dict = {}\n",
    "for key in scaled_data_dict:\n",
    "    lim_feat_sdata_dict[key] = {}\n",
    "    for feat in scaled_data_dict[key]:\n",
    "        if feat in feat_list:\n",
    "            lim_feat_sdata_dict[key][feat]= scaled_data_dict[key][feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = featureFormat(lim_feat_sdata_dict, feat_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(sdata)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, \n",
    "                                                                            test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-e9716721d303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 638\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 random_state)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# Early termination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \"\"\"\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SAMME.R'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;34m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/ensemble/base.pyc\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0m_set_random_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/ensemble/base.pyc\u001b[0m in \u001b[0;36m_set_random_states\u001b[0;34m(estimator, random_state)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mto_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;31m# Simple optimization to gain speed (inspect is slow)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mvalid_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m    226\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0;31m# We need deprecation warnings to always be on in order to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# catch deprecated param values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36m_get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# introspect the constructor arguments to find the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# to represent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0minit_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;31m# Consider the constructor parameters excluding 'self'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         parameters = [p for p in init_signature.parameters.values()\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/funcsigs.pyc\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__self__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# Unbound method: the first parameter becomes positional-only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/funcsigs.pyc\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/funcsigs.pyc\u001b[0m in \u001b[0;36mfrom_function\u001b[0;34m(cls, func)\u001b[0m\n\u001b[1;32m    578\u001b[0m         return cls(parameters,\n\u001b[1;32m    579\u001b[0m                    \u001b[0mreturn_annotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                    __validate_parameters__=False)\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/externals/funcsigs.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parameters, return_annotation, __validate_parameters__)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 params = OrderedDict(((param.name, param)\n\u001b[0;32m--> 504\u001b[0;31m                                                 for param in parameters))\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_setitem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jedfarm/anaconda/lib/python2.7/_abcoll.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "              \"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"base_estimator__max_depth\" : [None, 1, 2, 3, 4],\n",
    "              \"base_estimator__min_samples_leaf\" : [1, 2, 3, 4], \n",
    "              \"base_estimator__min_samples_split\" : [2, 3, 4, 5, 6],\n",
    "              \"n_estimators\": [50, 200, 400, 600, 800],\n",
    "              \"learning_rate\": [0.01, 0.1, 1]\n",
    "             }\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state = 45, class_weight = None, min_weight_fraction_leaf = 0)\n",
    "ABC = AdaBoostClassifier(base_estimator = DTC)\n",
    "\n",
    "# run grid search\n",
    "clf = GridSearchCV(ABC, param_grid=param_grid, scoring = 'f1')\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "print(clf.best_params_)\n",
    "\n",
    "print()\n",
    "print ('It took', time.time()-start, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator__criterion': 'gini',\n",
       " 'base_estimator__max_depth': 1,\n",
       " 'base_estimator__min_samples_leaf': 3,\n",
       " 'base_estimator__min_samples_split': 2,\n",
       " 'base_estimator__splitter': 'random',\n",
       " 'learning_rate': 0.1,\n",
       " 'n_estimators': 600}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=45,\n",
      "            splitter='random'),\n",
      "          learning_rate=0.1, n_estimators=600, random_state=None)\n",
      "\tAccuracy: 0.89473\tPrecision: 0.62274\tRecall: 0.53400\tF1: 0.57497\tF2: 0.54967\n",
      "\tTotal predictions: 15000\tTrue positives: 1068\tFalse positives:  647\tFalse negatives:  932\tTrue negatives: 12353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(criterion = 'gini', max_depth = 1, min_samples_leaf = 3, min_samples_split = 2, \n",
    "                            splitter = 'random', random_state = 45)\n",
    "clf = AdaBoostClassifier(base_estimator = dtc, learning_rate = 0.1, n_estimators = 600)\n",
    "tester.dump_classifier_and_data(clf, lim_feat_sdata_dict, feat_list)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right out the box, only the Gaussian Naive Bayes classifier failed to meet the minimum requirement of 0.3 or higher in Accuracy, Recall and Precision. Let see if that could be fixed with scaling and changing the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three classifiers, data scaled, reduced number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a collection of features, sometimes their values vary within wild margins. In our case, the changes in email related features will be in the range of thousands at the maximum. Meanwhile, the financial features could change several orders of magnitude above that. This fact could cause those financial features to become more relevant than the email-related ones just because of their size. To correct for this, we applied a feature (standard) scaling procedure to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "s_scaled = scaler.fit_transform(df.iloc[:,1:])\n",
    "scaled_df = pd.DataFrame(s_scaled, index=df.index)\n",
    "scaled_df.insert(0, \"poi\", df.poi)\n",
    "scaled_df.columns = features_list\n",
    "scaled_data_dict = scaled_df.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is well-known that a large number of features might have a negative impact on the overall performance of a model, with a tendency to generate overfitting. Next, we will try to reduce the dimensionality of our data in different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "for nc in range(1, scaled_df.shape[1]-1):\n",
    "    my_model = PCA(n_components=nc)\n",
    "    ft = my_model.fit_transform(scaled_df.iloc[:,1:])\n",
    "    if my_model.explained_variance_ratio_.cumsum()[-1] >= 0.9:\n",
    "            pca_df = pd.DataFrame(ft,  index=scaled_df.index)\n",
    "            pca_df.insert(0, \"poi\", scaled_df.poi)\n",
    "            pca_data_dict = pca_df.to_dict(orient = 'index')\n",
    "            pca_feat_list = ['poi'] + list(range(nc))\n",
    "            print(\"Number of components: \", nc)\n",
    "            print (\"Retain\", my_model.explained_variance_ratio_.cumsum()[-1] * 100, \"% of the variance\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, pca_data_dict, pca_feat_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA), we optimized our number of features (-PCA features are different from the ones we have already defined-) in such a way we retained at least 90% of the variance. That was barely enough to create a project compliant Naive Bayes classifier. \n",
    "\n",
    "We found the optimal number of features manually in the case of the Naive Bayes classifier using KBest as our feature selection method, which produced slightly better results than PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kbest_feat = 8\n",
    "selector = SelectKBest(f_classif, k=kbest_feat)\n",
    "select_k_best_classifier = selector.fit_transform(s_scaled , df.poi)\n",
    "scores = selector.scores_\n",
    "mask = selector.get_support() \n",
    "kselect_features = [] \n",
    "feat_importance = []\n",
    "for bool, feature, score in zip(mask, cols[1:], scores):\n",
    "    if bool:\n",
    "        kselect_features.append(feature)\n",
    "        feat_importance.append([feature, round(score, 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "kselect_df = pd.DataFrame(select_k_best_classifier, index = df.index, columns = kselect_features)\n",
    "kselect_df.insert(0, \"poi\", df.poi)\n",
    "kselect_data_dict = kselect_df.to_dict(orient = 'index')  \n",
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, kselect_data_dict, [\"poi\"] + kselect_features )\n",
    "tester.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.1:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "tree_feat_list = [x[0] for x in feat_importance]\n",
    "tree_feat_list.insert(0, 'poi')\n",
    "tree_lim_df = scaled_df[tree_feat_list]\n",
    "lim_feat_data_dict = tree_lim_df.to_dict(orient = 'index')\n",
    "\n",
    "dump_classifier_and_data(clf, lim_feat_data_dict, tree_feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.02:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "feat_list = [x[0] for x in feat_importance]\n",
    "feat_list.insert(0, 'poi')\n",
    "lim_feat_sdata_dict = {}\n",
    "for key in scaled_data_dict:\n",
    "    lim_feat_sdata_dict[key] = {}\n",
    "    for feat in scaled_data_dict[key]:\n",
    "        if feat in feat_list:\n",
    "            lim_feat_sdata_dict[key][feat]= scaled_data_dict[key][feat]\n",
    "dump_classifier_and_data(clf, lim_feat_sdata_dict, feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, after performing some feature scaling and selection, all three classifiers met the minimum requirements of this project. In spite of the fact that Naive Bayes displayed the best performance increase, it still falls short in comparison to the other two classifiers.\n",
    "\n",
    "Before going further with algorithm tuning, let proceed to create a collection of new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the features and running the classifiers (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "s_scaled = scaler.fit_transform(df.iloc[:,1:])\n",
    "scaled_df = pd.DataFrame(s_scaled, index=df.index)\n",
    "scaled_df.insert(0, \"poi\", df.poi)\n",
    "scaled_df.columns = full_feat_cols\n",
    "scaled_data_dict = scaled_df.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_feat = 9\n",
    "selector = SelectKBest(f_classif, k=kbest_feat)\n",
    "select_k_best_classifier = selector.fit_transform(s_scaled , df.poi)\n",
    "scores = selector.scores_\n",
    "mask = selector.get_support() \n",
    "kselect_features = [] \n",
    "feat_importance = []\n",
    "for bool, feature, score in zip(mask, full_feat_cols[1:], scores):\n",
    "    if bool:\n",
    "        kselect_features.append(feature)\n",
    "        feat_importance.append([feature, round(score, 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "kselect_df = pd.DataFrame(select_k_best_classifier, index = df.index, columns = kselect_features)\n",
    "kselect_df.insert(0, \"poi\", df.poi)\n",
    "kselect_data_dict = kselect_df.to_dict(orient = 'index')  \n",
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, kselect_data_dict, [\"poi\"] + kselect_features)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.1:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "tree_feat_list = [x[0] for x in feat_importance]\n",
    "tree_feat_list.insert(0, 'poi')\n",
    "tree_lim_df = scaled_df[tree_feat_list]\n",
    "lim_feat_data_dict = tree_lim_df.to_dict(orient = 'index')\n",
    "\n",
    "dump_classifier_and_data(clf, lim_feat_data_dict, tree_feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.02:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "feat_list = [x[0] for x in feat_importance]\n",
    "feat_list.insert(0, 'poi')\n",
    "lim_feat_sdata_dict = {}\n",
    "for key in scaled_data_dict:\n",
    "    lim_feat_sdata_dict[key] = {}\n",
    "    for feat in scaled_data_dict[key]:\n",
    "        if feat in feat_list:\n",
    "            lim_feat_sdata_dict[key][feat]= scaled_data_dict[key][feat]\n",
    "dump_classifier_and_data(clf, lim_feat_sdata_dict, feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all the classifiers reacted in the same way to the addition of the new features. The Decision Tree classifier was the one that improved the most. For that reason, and because it is faster than the AdaBoost classifier, we picked it for further optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHM TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing Parameters. Decision Tree Classifier\n",
    "param_grid = dict(criterion = ['gini', 'entropy'] , \n",
    "                  min_samples_split = [2, 4, 8, 12, 16, 20, 24, 26, 28],\n",
    "                  max_depth = [None, 1, 2, 3, 4, 5, 6, 7],\n",
    "                  max_features = [None, 'sqrt', 'log2', 'auto'])\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state = 45), param_grid = param_grid, cv=10,\n",
    "                       scoring='f1')\n",
    "clf.fit(tree_lim_df.iloc[:,1:], tree_lim_df.poi)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion = 'entropy', max_depth = None, max_features = None, \n",
    "                            min_samples_split = 20)\n",
    "dump_classifier_and_data(clf, lim_feat_data_dict, tree_feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tunning the Decision Tree Classifier parameters, we obtained decent values for all the relevant metrics, (all of them at or above 0.7). It is worth mentioning that we tried this out using a fixed random state, if we remove this restriction, not only the optimal parameters change from one run to the next, but also the results given by tester.py. To be honest, we should expect our metrics to be a little lower (as an average) than the ones shown, but still well above the minimum acceptable value of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results\")\n",
    "pd.DataFrame([[0.93, 0.71, 0.81, 0.76]],\n",
    "             columns = ['Accuracy','Precision', 'Recall', 'F1'], \n",
    "             index = ['Decision Tree Classifier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS\n",
    "\n",
    "We applied three well-known machine learning algorithms to a combination of financial data (FindLaw) and email-related data from the Enron dataset in an attempt to find persons of interest (poi) in the Enron scandal case.\n",
    "As it is almost always the case in Data Analysis, the preprocessing of the data played an essential role in the development and final results of our project.\n",
    "\n",
    "The data set provided contained information about 144 people involved (18 of them were poi); which is, by all means, a small amount of data for the intended task. With an initial assessment of 1323 missing entries within which there were 25 \"strange cases\" of missing email information, the prospects for success were not precisely enjoyable. Fortunately, we learned that the missing (NaN) values in the financial data were in reality zeros and that dramatically reduced our missing entries to a more manageable amount of 290.  After that, and making use of the Enron email dataset, we were able to unravel the mystery of those 25 missing cases and decided in correspondence. We chose to impute the NaN values with the median of their respective columns making the difference between poi and non-poi. That decision was not taken blindly. It was such that it maximized the performance of our classifiers.\n",
    "\n",
    "After applying Gaussian Naive Bayes, Decision Tree and AdaBoost classifiers to a reduced number of features of the previously scaled data, we observed that all three of them fulfilled the minimum requirements for the project, with a slight advantage for Decision Tree. The incorporation of the new features ended up tipping the balance to the Decision Tree Classifier. We tuned this classifier using GridSearchCV and a definite random_state to make our results consistent from one run to the next. In the end, after applying tester.py our best values were as follow: Accuracy: 0.93, Precision: 0.71, Recall 0.81 and F1 0.76. \n",
    "We believe that this set of values is indicative of a solid performance by the Decision Tree classifier in this problem, but there is still room for further improvements. \n",
    "\n",
    "\n",
    "## Limitations and future work\n",
    "\n",
    "As we mentioned above, one of the inherent limitations of this project emanates from the small size of the data set, just 144 people (18 poi). The quality of the data also played a significant role, as the government intervention for privacy issues substantially modified part of the data making them useless for this study.\n",
    "What is exciting about this project is that by using the full email data set, it is possible to create, at least in principle, any number of new and meaningful features. I believe that if we work this process in more detail,  we could end up creating features that could improve further the efficiency of our classifiers. For instance, some features could take into account the flux of emails around the critical dates, or be the result of sentiment analysis applied to the emails texts. This is an endeavor I will happily pursue in the future.\n",
    "\n",
    "\n",
    "# REFERENCES\n",
    "\n",
    "1. http://www.ahschulz.de/enron-email-data/\n",
    "2. https://enrondata.readthedocs.io/en/latest/data/custodian-names-and-titles/\n",
    "3. http://www.infosys.tuwien.ac.at/staff/dschall/email/enron-employees.txt\n",
    "4. https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/\n",
    "5. https://codereview.stackexchange.com/questions/146834/function-to-find-all-occurrences-of-substring\n",
    "6. https://regex101.com\n",
    "7. https://stackoverflow.com/questions/32468402/how-to-explode-a-list-inside-a-dataframe-cell-into-separate-rows\n",
    "\n",
    "8. https://rodgersnotes.wordpress.com/2013/11/19/enron-email-analysis-persons-of-interest/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
