{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../tools/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jedfarm/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.decomposition import PCA\n",
    "import tester\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"rb\"))\n",
    "df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that all the email-related data were collected using valid email addresses, therefore if some people have their email addresses missing, that implies the corresponding email features will be missing as well (NaN). The contrary is not true as we are about to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with missing email data:  60\n",
      "Number of people with missing email address:  35\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of people with missing email data: \",df['from_messages'].value_counts().max())\n",
    "print(\"Number of people with missing email address: \",df['email_address'].value_counts().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are these people?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People with at least one email address but without email-related features: \n",
      "\n",
      "1 ELLIOTT STEVEN\n",
      "2 MORDAUNT KRISTINA M\n",
      "3 WESTFAHL RICHARD K\n",
      "4 WODRASKA JOHN\n",
      "5 ECHOLS JOHN B\n",
      "6 KOPPER MICHAEL J\n",
      "7 BERBERIAN DAVID\n",
      "8 DETMERING TIMOTHY J\n",
      "9 GOLD JOSEPH\n",
      "10 KISHKILL JOSEPH G\n",
      "11 LINDHOLM TOD A\n",
      "12 BUTTS ROBERT H\n",
      "13 HERMANN ROBERT J\n",
      "14 SCRIMSHAW MATTHEW\n",
      "15 FASTOW ANDREW S\n",
      "16 OVERDYKE JR JERE C\n",
      "17 STABLER FRANK\n",
      "18 PRENTICE JAMES\n",
      "19 WHITE JR THOMAS E\n",
      "20 CHRISTODOULOU DIOMEDES\n",
      "21 DIMICHELE RICHARD G\n",
      "22 YEAGER F SCOTT\n",
      "23 HIRKO JOSEPH\n",
      "24 PAI LOU L\n",
      "25 BAY FRANKLIN R\n"
     ]
    }
   ],
   "source": [
    "extrange_cases = df.index[(df['email_address'] != 'NaN') & (df['from_messages']=='NaN')].tolist()\n",
    "print(\"People with at least one email address but without email-related features: \")\n",
    "print()\n",
    "for i, name in enumerate(extrange_cases):\n",
    "    print(i + 1, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are 25 extrange cases, where actual Enron employees with a valid email address,  do not have email-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People without an email address: \n",
      "\n",
      "1 BAXTER JOHN C\n",
      "2 LOWRY CHARLES P\n",
      "3 WALTERS GARETH W\n",
      "4 CHAN RONNIE\n",
      "5 BELFER ROBERT\n",
      "6 URQUHART JOHN A\n",
      "7 WHALEY DAVID A\n",
      "8 MENDELSOHN JOHN\n",
      "9 CLINE KENNETH W\n",
      "10 WAKEHAM JOHN\n",
      "11 DUNCAN JOHN H\n",
      "12 LEMAISTRE CHARLES\n",
      "13 SULLIVAN-SHAKLOVITZ COLLEEN\n",
      "14 WROBEL BRUCE\n",
      "15 MEYER JEROME J\n",
      "16 CUMBERLAND MICHAEL S\n",
      "17 GAHN ROBERT S\n",
      "18 GATHMANN WILLIAM D\n",
      "19 GILLIS JOHN\n",
      "20 BAZELIDES PHILIP J\n",
      "21 LOCKHART EUGENE E\n",
      "22 PEREIRA PAULO V. FERRAZ\n",
      "23 BLAKE JR. NORMAN P\n",
      "24 GRAY RODNEY\n",
      "25 THE TRAVEL AGENCY IN THE PARK\n",
      "26 NOLES JAMES L\n",
      "27 TOTAL\n",
      "28 JAEDICKE ROBERT\n",
      "29 WINOKUR JR. HERBERT S\n",
      "30 BADUM JAMES P\n",
      "31 REYNOLDS LAWRENCE\n",
      "32 YEAP SOON\n",
      "33 FUGH JOHN L\n",
      "34 SAVAGE FRANK\n",
      "35 GRAMM WENDY L\n"
     ]
    }
   ],
   "source": [
    "emailless_people = df.index[df['email_address'] == 'NaN'].tolist()\n",
    "print(\"People without an email address: \")\n",
    "print()\n",
    "for i, name in enumerate(emailless_people):\n",
    "    print(i + 1, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we found two entities, that are not real people: THE TRAVEL AGENCY IN THE PARK and TOTAL. These entities do not contribute in any meaningful way to the purpose of this study, so let say that from this moment on we mark them for deletion. \n",
    "\n",
    "The email address is the only field that could not be converted to numeric. We chose to remove it from the data frame because it is of no use to identify poi from the data given.\n",
    "Also, in the case of the poi column only zeroes (0) and ones (1) are allowed: 1 = poi, 0 = non-poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.apply(lambda x: pd.to_numeric(x, errors='coerse'))\n",
    "del df['email_address']\n",
    "df['poi']=df['poi'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, METTS MARK to GLISAN JR BEN F\n",
      "Data columns (total 20 columns):\n",
      "poi                          146 non-null int64\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "shared_receipt_with_poi      86 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "dtypes: float64(19), int64(1)\n",
      "memory usage: 24.0+ KB\n"
     ]
    }
   ],
   "source": [
    "poi_label = ['poi']\n",
    "financial_feat_list = ['salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments', 'loan_advances', \n",
    "        'other', 'expenses', 'director_fees', 'total_payments', 'exercised_stock_options', 'restricted_stock',\n",
    "        'restricted_stock_deferred', 'total_stock_value']\n",
    "email_feat_list = ['from_messages', 'from_poi_to_this_person',\n",
    "        'from_this_person_to_poi', 'shared_receipt_with_poi', 'to_messages']\n",
    "\n",
    "features_list = poi_label + financial_feat_list + email_feat_list\n",
    "\n",
    "# cols = ['poi', 'salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments', 'loan_advances', \n",
    "#         'other', 'expenses', 'director_fees', 'total_payments', 'exercised_stock_options', 'restricted_stock',\n",
    "#         'restricted_stock_deferred', 'total_stock_value', 'from_messages', 'from_poi_to_this_person',\n",
    "#         'from_this_person_to_poi', 'shared_receipt_with_poi', 'to_messages']\n",
    "df=df[features_list]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of poi and non-poi in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    128\n",
       "1     18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a relatively low number of data points, we have to proceed extra-carefully at removing them.\n",
    "For the moment, we are going to do it just to the items we previously have marked for deletion, and we will analyze any further need in a case by case manner as we proceed with our ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['TOTAL'])\n",
    "df=df.drop(['THE TRAVEL AGENCY IN THE PARK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the financial data, we learned that NaN means zero. Therefore we proceed to make the corresponding changes in our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 1:15] = df.iloc[:, 1:15].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing such an operation, the number of NaN values was dramatically reduced from 1323 up to 290, which ultimately is the amount of missing email-related entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of email-related missing data:  290\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of email-related missing data: \", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few amount of data available. In the case of poi, there are just 18 people in our dataset, four of them (over 20%) did not have email-related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of poi without email data:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of poi without email data: \", df[(df['poi']==1) & (~df.to_messages.notnull())].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the best way to proceed with the remaining NaN values is to impute them with the median for non-poi people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of remaining NaN entries in the dataframe: 0\n"
     ]
    }
   ],
   "source": [
    "#email_cols = cols[15:]\n",
    "df[email_feat_list]=df[email_feat_list].fillna(df.groupby(\"poi\")[email_feat_list].transform(\"median\"))\n",
    "print(\"Amount of remaining NaN entries in the dataframe:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As found in some of our references, the manual input of the financial data could have been the cause of some observed mistakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT        0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  \n",
       "BELFER ROBERT              -44093.0  \n",
       "BHATNAGAR SANJAY                0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payments = financial_feat_list[:9]\n",
    "df[df[payments].sum(axis = 1) != df.total_payments][financial_feat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT        0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  \n",
       "BELFER ROBERT              -44093.0  \n",
       "BHATNAGAR SANJAY                0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_value = financial_feat_list[10:13]\n",
    "test_df=df[df[stock_value].sum(axis='columns') != df.total_stock_value][financial_feat_list]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there are errors in just two rows. Checking the .pdf document obtained from FindLaw, we acknowledged that the errors are in fact shifts of one column in each case but opposite directions. Let's correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the financial data has been corrected\n"
     ]
    }
   ],
   "source": [
    "test_df.loc[['BELFER ROBERT']] = test_df.loc[['BELFER ROBERT']].shift(-1, axis =1).fillna(0)\n",
    "test_df.loc[['BHATNAGAR SANJAY']] = test_df.loc[['BHATNAGAR SANJAY']].shift(1, axis =1).fillna(0)\n",
    "\n",
    "df.update(test_df)\n",
    "\n",
    "if not (df[df[payments].sum(axis = 1) != df.total_payments].shape[0] | df[df[stock_value].sum(\n",
    "    axis='columns') != df.total_stock_value][financial_feat_list].shape[0]):\n",
    "    print(\"All the financial data has been corrected\")\n",
    "else:\n",
    "    print(\"Some errors remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL ALGORITHM TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three classifiers, data as is, all features.\n",
    "\n",
    "We chose to work with Naive Bayes, Decision Tree, and AdaBoost classifiers. Initially, we run all of them with the data as is and all the existing features in our data set. The results are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dict = df.to_dict(orient = 'index')\n",
    "clf = GaussianNB()\n",
    "tester.dump_classifier_and_data(clf, raw_data_dict, cols)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 45)\n",
    "tester.dump_classifier_and_data(clf, raw_data_dict, cols)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state = 45)\n",
    "tester.dump_classifier_and_data(clf, raw_data_dict, cols)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right out the box, only the Gaussian Naive Bayes classifier failed to meet the minimum requirement of 0.3 or higher in Accuracy, Recall and Precision. Let see if that could be fixed with scaling and changing the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three classifiers, data scaled, reduced number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a collection of features, sometimes their values vary within wild margins. In our case, the changes in email related features will be in the range of thousands at the maximum. Meanwhile, the financial features could change several orders of magnitude above that. This fact could cause those financial features to become more relevant than the email-related ones just because of their size. To correct for this, we applied a feature (standard) scaling procedure to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "s_scaled = scaler.fit_transform(df.iloc[:,1:])\n",
    "scaled_df = pd.DataFrame(s_scaled, index=df.index)\n",
    "scaled_df.insert(0, \"poi\", df.poi)\n",
    "scaled_df.columns = cols\n",
    "scaled_data_dict = scaled_df.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is well-known that a large number of features might have a negative impact on the overall performance of a model, with a tendency to generate overfitting. Next, we will try to reduce the dimensionality of our data in different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "for nc in range(1, scaled_df.shape[1]-1):\n",
    "    my_model = PCA(n_components=nc)\n",
    "    ft = my_model.fit_transform(scaled_df.iloc[:,1:])\n",
    "    if my_model.explained_variance_ratio_.cumsum()[-1] >= 0.9:\n",
    "            pca_df = pd.DataFrame(ft,  index=scaled_df.index)\n",
    "            pca_df.insert(0, \"poi\", scaled_df.poi)\n",
    "            pca_data_dict = pca_df.to_dict(orient = 'index')\n",
    "            pca_feat_list = ['poi'] + list(range(nc))\n",
    "            print(\"Number of components: \", nc)\n",
    "            print (\"Retain\", my_model.explained_variance_ratio_.cumsum()[-1] * 100, \"% of the variance\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, pca_data_dict, pca_feat_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA), we optimized our number of features (-PCA features are different from the ones we have already defined-) in such a way we retained at least 90% of the variance. That was barely enough to create a project compliant Naive Bayes classifier. \n",
    "\n",
    "We found the optimal number of features manually in the case of the Naive Bayes classifier using KBest as our feature selection method, which produced slightly better results than PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kbest_feat = 8\n",
    "selector = SelectKBest(f_classif, k=kbest_feat)\n",
    "select_k_best_classifier = selector.fit_transform(s_scaled , df.poi)\n",
    "scores = selector.scores_\n",
    "mask = selector.get_support() \n",
    "kselect_features = [] \n",
    "feat_importance = []\n",
    "for bool, feature, score in zip(mask, cols[1:], scores):\n",
    "    if bool:\n",
    "        kselect_features.append(feature)\n",
    "        feat_importance.append([feature, round(score, 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "kselect_df = pd.DataFrame(select_k_best_classifier, index = df.index, columns = kselect_features)\n",
    "kselect_df.insert(0, \"poi\", df.poi)\n",
    "kselect_data_dict = kselect_df.to_dict(orient = 'index')  \n",
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, kselect_data_dict, [\"poi\"] + kselect_features )\n",
    "tester.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.1:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "tree_feat_list = [x[0] for x in feat_importance]\n",
    "tree_feat_list.insert(0, 'poi')\n",
    "tree_lim_df = scaled_df[tree_feat_list]\n",
    "lim_feat_data_dict = tree_lim_df.to_dict(orient = 'index')\n",
    "\n",
    "dump_classifier_and_data(clf, lim_feat_data_dict, tree_feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.02:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "feat_list = [x[0] for x in feat_importance]\n",
    "feat_list.insert(0, 'poi')\n",
    "lim_feat_sdata_dict = {}\n",
    "for key in scaled_data_dict:\n",
    "    lim_feat_sdata_dict[key] = {}\n",
    "    for feat in scaled_data_dict[key]:\n",
    "        if feat in feat_list:\n",
    "            lim_feat_sdata_dict[key][feat]= scaled_data_dict[key][feat]\n",
    "dump_classifier_and_data(clf, lim_feat_sdata_dict, feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, after performing some feature scaling and selection, all three classifiers met the minimum requirements of this project. In spite of the fact that Naive Bayes displayed the best performance increase, it still falls short in comparison to the other two classifiers.\n",
    "\n",
    "Before going further with algorithm tuning, let proceed to create a collection of new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE NEW FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most straightforward manner of creating new features, in this case, is by using the existing ones. For example, we can create meaningful ratios of two features. A more complicated way to achieve the same goal is to work extensively with the full Enron email dataset. As we were curious about those cases of existing emails and no email related data, we decided to dive into the Enron email data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the mysterious missing data\n",
    "\n",
    "Exploring the Enron email dataset proved to be a time-consuming task. After searching with an intricate pattern of regular expressions and using specific search criteria based on the observed email addresses patterns, we were able to find up to 424 different email addresses linked to the people under study. Our search methods were far from optimal as they included final manual adjudications in many cases. That is why we have reasons to believe that there could be more email addresses than the ones we were able to find (but we decided to leave that as a subject of a more detailed study to be carried out in the future). In any case, our search allowed us to find some of the missing email addresses, and with that information, we built the email-based existing features for the employees including those \"strange 25 cases\". The code is too large to be inserted here, but we provide a text file with the procedure followed alongside with the script files we used. We are going to load a dictionary we created, similar to data_dict in structure, but with the data we processed directly from the Enron email dataset. It also contains new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dict = pickle.load(open(\"../final_project/new_data_dict.pickle\", \"rb\"))\n",
    "df_new = pd.DataFrame.from_records(list(new_data_dict.values()))\n",
    "employees = pd.Series(list(new_data_dict.keys()))\n",
    "# set the index of df_new to be the employees series:\n",
    "df_new.set_index(employees, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrange_missing_data = df_new[df_new.index.isin(extrange_cases)][cols[15:]]\n",
    "#print(\"Number of people with email data recovered\", missing_data.shape[0])\n",
    "extrange_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mystery solved: We were able to find the email-related data belonging to those 25 people with valid email addresses. Taking a quick look at the data we realized that there is a disproportion between the number of emails sent and received, the amount of messages sent by these people is suspiciously low (or inexistent) for the timeframe considered. We found email data for additional 19 people (from our second list of 35 shown above) that displays the same trend. \n",
    "\n",
    "One particular case in the above data frame is worth noticing: Andrew S. Fastow. It is hard to believe that the chief financial officer of a corporation, (who received at least 1183 emails) just sent nine emails in more than a year, including the time when the financial scandal shattered the company.\n",
    "\n",
    "I believe that the process of emails removal from the dataset due to privacy protection issues that occurred at some point after the first release of the Enron email data might have something to do with this. As this is an intentional intervention in the data set, it definitively could affect the outcome of any attempt of classification if these data were to be included. It is, therefore, reasonable to assume that this particular situation is the reason behind the absence of the email-related features for those 25 \"strange cases\" we found earlier. \n",
    "\n",
    "### New features\n",
    "\n",
    "Having the Enron email dataset in a workable shape makes easy to create any number of new features. We believe the simpler they are, the better. For that reason, we created two different kinds of new features. Some were ratios of the existing features as we mentioned earlier, and others were the result of working with the entire email dataset. \n",
    "In the second case, we created an intermediate feature, called pubIndex.  This one is not going to be used explicitly, but it is involved in computing most of the new features.\n",
    "\n",
    " pubIndex accounts for the number of people involved in a given email (To and Cc fields) correcting for when people sent emails to themselves. The lowest possible value for this feature is zero (if someone sent an email just to him or herself with no Cc), it is equal to one if there is only a single person in the To field and none in the Cc field, and so on. It is worth noticing that there is in principle no upper limit for this feature.  \n",
    "\n",
    "### Ratios of existing features:\n",
    "\n",
    "- to_poi_rate: ratio of from_this_person_to_poi / from_messages\n",
    "\n",
    "- from_poi_rate: ratio of from_poi_to_this_person / to_messages\n",
    "\n",
    "### New features from the email dataset:\n",
    "\n",
    "- from_poi_cc_this_person: This is the complement of the existent from_poi_to_this_person, it takes into account the Cc field of the emails sent from poi accounts, it was corrected for any email that poi sent (Cc) to themselves.\n",
    "\n",
    "- median_from_poi_cc_this_person_pubIndex: We grouped all the emails sent from poi with Cc to a given person and calculated the median of the pubIndex feature.\n",
    "\n",
    "- median_from_poi_to_this_person_pubIndex: The same as above but for the field To.\n",
    "\n",
    "- median_cc_this_person_pubIndex: We grouped all the emails where a person was in the Cc field and calculated the median of the pubIndex feature.\n",
    "\n",
    "- median_from_this_person_pubIndex: We grouped all the emails sent by a given person and took the median of the pubIndex feature.\n",
    "\n",
    "The rest of the new features could be easily understood.\n",
    "\n",
    "- median_from_this_person_cc_poi_pubIndex\n",
    "\n",
    "- median_from_this_person_to_poi_pubIndex\n",
    "\n",
    "- median_to_this_person_pubIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['to_poi_rate'] = df['from_this_person_to_poi']/df['from_messages']\n",
    "df['from_poi_rate'] = df['from_poi_to_this_person']/df['to_messages']\n",
    "new_feat = ['from_poi_cc_this_person', 'median_from_poi_cc_this_person_pubIndex', \n",
    "            'median_from_poi_to_this_person_pubIndex', 'median_cc_this_person_pubIndex', \n",
    "            'median_from_this_person_pubIndex','median_from_this_person_cc_poi_pubIndex', \n",
    "           'median_from_this_person_to_poi_pubIndex', 'median_to_this_person_pubIndex']\n",
    "nf_df = df_new[new_feat]\n",
    "nf_df=nf_df.apply(lambda x: pd.to_numeric(x, errors='coerse'))\n",
    "df = pd.concat([df, nf_df], axis=1)\n",
    "df[new_feat]=df[new_feat].fillna(df.groupby(\"poi\")[new_feat].transform(\"median\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of email-related missing data: \", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feat_cols = cols + ['to_poi_rate', 'from_poi_rate'] + new_feat\n",
    "#df = df[full_feat_cols]\n",
    "print(\"Total number of features: \", len(full_feat_cols)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the features and running the classifiers (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "s_scaled = scaler.fit_transform(df.iloc[:,1:])\n",
    "scaled_df = pd.DataFrame(s_scaled, index=df.index)\n",
    "scaled_df.insert(0, \"poi\", df.poi)\n",
    "scaled_df.columns = full_feat_cols\n",
    "scaled_data_dict = scaled_df.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_feat = 9\n",
    "selector = SelectKBest(f_classif, k=kbest_feat)\n",
    "select_k_best_classifier = selector.fit_transform(s_scaled , df.poi)\n",
    "scores = selector.scores_\n",
    "mask = selector.get_support() \n",
    "kselect_features = [] \n",
    "feat_importance = []\n",
    "for bool, feature, score in zip(mask, full_feat_cols[1:], scores):\n",
    "    if bool:\n",
    "        kselect_features.append(feature)\n",
    "        feat_importance.append([feature, round(score, 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "kselect_df = pd.DataFrame(select_k_best_classifier, index = df.index, columns = kselect_features)\n",
    "kselect_df.insert(0, \"poi\", df.poi)\n",
    "kselect_data_dict = kselect_df.to_dict(orient = 'index')  \n",
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, kselect_data_dict, [\"poi\"] + kselect_features)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.1:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "tree_feat_list = [x[0] for x in feat_importance]\n",
    "tree_feat_list.insert(0, 'poi')\n",
    "tree_lim_df = scaled_df[tree_feat_list]\n",
    "lim_feat_data_dict = tree_lim_df.to_dict(orient = 'index')\n",
    "\n",
    "dump_classifier_and_data(clf, lim_feat_data_dict, tree_feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state = 45)\n",
    "clf.fit(scaled_df.iloc[:,1:], scaled_df[\"poi\"])\n",
    "\n",
    "feat_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.02:\n",
    "        feat_importance.append([scaled_df.columns[i+1], round(clf.feature_importances_[i], 2)])\n",
    "feat_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for item in feat_importance:\n",
    "    print('{} ===> {}'.format(item[0], item[1]))\n",
    "print()\n",
    "feat_list = [x[0] for x in feat_importance]\n",
    "feat_list.insert(0, 'poi')\n",
    "lim_feat_sdata_dict = {}\n",
    "for key in scaled_data_dict:\n",
    "    lim_feat_sdata_dict[key] = {}\n",
    "    for feat in scaled_data_dict[key]:\n",
    "        if feat in feat_list:\n",
    "            lim_feat_sdata_dict[key][feat]= scaled_data_dict[key][feat]\n",
    "dump_classifier_and_data(clf, lim_feat_sdata_dict, feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all the classifiers reacted in the same way to the addition of the new features. The Decision Tree classifier was the one that improved the most. For that reason, and because it is faster than the AdaBoost classifier, we picked it for further optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHM TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing Parameters. Decision Tree Classifier\n",
    "param_grid = dict(criterion = ['gini', 'entropy'] , \n",
    "                  min_samples_split = [2, 4, 8, 12, 16, 20, 24, 26, 28],\n",
    "                  max_depth = [None, 1, 2, 3, 4, 5, 6, 7],\n",
    "                  max_features = [None, 'sqrt', 'log2', 'auto'])\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state = 45), param_grid = param_grid, cv=10,\n",
    "                       scoring='f1')\n",
    "clf.fit(tree_lim_df.iloc[:,1:], tree_lim_df.poi)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion = 'entropy', max_depth = None, max_features = None, \n",
    "                            min_samples_split = 20)\n",
    "dump_classifier_and_data(clf, lim_feat_data_dict, tree_feat_list )\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tunning the Decision Tree Classifier parameters, we obtained decent values for all the relevant metrics, (all of them at or above 0.7). It is worth mentioning that we tried this out using a fixed random state, if we remove this restriction, not only the optimal parameters change from one run to the next, but also the results given by tester.py. To be honest, we should expect our metrics to be a little lower (as an average) than the ones shown, but still well above the minimum acceptable value of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results\")\n",
    "pd.DataFrame([[0.93, 0.71, 0.81, 0.76]],\n",
    "             columns = ['Accuracy','Precision', 'Recall', 'F1'], \n",
    "             index = ['Decision Tree Classifier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS\n",
    "\n",
    "We applied three well-known machine learning algorithms to a combination of financial data (FindLaw) and email-related data from the Enron dataset in an attempt to find persons of interest (poi) in the Enron scandal case.\n",
    "As it is almost always the case in Data Analysis, the preprocessing of the data played an essential role in the development and final results of our project.\n",
    "\n",
    "The data set provided contained information about 144 people involved (18 of them were poi); which is, by all means, a small amount of data for the intended task. With an initial assessment of 1323 missing entries within which there were 25 \"strange cases\" of missing email information, the prospects for success were not precisely enjoyable. Fortunately, we learned that the missing (NaN) values in the financial data were in reality zeros and that dramatically reduced our missing entries to a more manageable amount of 290.  After that, and making use of the Enron email dataset, we were able to unravel the mystery of those 25 missing cases and decided in correspondence. We chose to impute the NaN values with the median of their respective columns making the difference between poi and non-poi. That decision was not taken blindly. It was such that it maximized the performance of our classifiers.\n",
    "\n",
    "After applying Gaussian Naive Bayes, Decision Tree and AdaBoost classifiers to a reduced number of features of the previously scaled data, we observed that all three of them fulfilled the minimum requirements for the project, with a slight advantage for Decision Tree. The incorporation of the new features ended up tipping the balance to the Decision Tree Classifier. We tuned this classifier using GridSearchCV and a definite random_state to make our results consistent from one run to the next. In the end, after applying tester.py our best values were as follow: Accuracy: 0.93, Precision: 0.71, Recall 0.81 and F1 0.76. \n",
    "We believe that this set of values is indicative of a solid performance by the Decision Tree classifier in this problem, but there is still room for further improvements. \n",
    "\n",
    "\n",
    "## Limitations and future work\n",
    "\n",
    "As we mentioned above, one of the inherent limitations of this project emanates from the small size of the data set, just 144 people (18 poi). The quality of the data also played a significant role, as the government intervention for privacy issues substantially modified part of the data making them useless for this study.\n",
    "What is exciting about this project is that by using the full email data set, it is possible to create, at least in principle, any number of new and meaningful features. I believe that if we work this process in more detail,  we could end up creating features that could improve further the efficiency of our classifiers. For instance, some features could take into account the flux of emails around the critical dates, or be the result of sentiment analysis applied to the emails texts. This is an endeavor I will happily pursue in the future.\n",
    "\n",
    "\n",
    "# REFERENCES\n",
    "\n",
    "1. http://www.ahschulz.de/enron-email-data/\n",
    "2. https://enrondata.readthedocs.io/en/latest/data/custodian-names-and-titles/\n",
    "3. http://www.infosys.tuwien.ac.at/staff/dschall/email/enron-employees.txt\n",
    "4. https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/\n",
    "5. https://codereview.stackexchange.com/questions/146834/function-to-find-all-occurrences-of-substring\n",
    "6. https://regex101.com\n",
    "7. https://stackoverflow.com/questions/32468402/how-to-explode-a-list-inside-a-dataframe-cell-into-separate-rows\n",
    "\n",
    "8. https://rodgersnotes.wordpress.com/2013/11/19/enron-email-analysis-persons-of-interest/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
